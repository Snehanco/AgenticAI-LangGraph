{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3e61cda",
   "metadata": {},
   "source": [
    "### üß† Answer Synthesis from Multiple Sources\n",
    "‚úÖ What Is It?\n",
    "\n",
    "Answer synthesis from multiple sources is the process where an AI agent collects information from different retrieval tools or knowledge bases, and merges that information into a single, coherent, and contextually rich answer.\n",
    "\n",
    "This is a core capability in Agentic RAG, where the system is more than just a simple retriever ‚Äî it plans, retrieves, and then synthesizes an answer that draws from multiple sources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e498f0",
   "metadata": {},
   "source": [
    "üéØ Why It‚Äôs Needed\n",
    "Most real-world queries are:\n",
    "- Multifaceted (require multiple types of information)\n",
    "- Ambiguous or incomplete (need refinement)\n",
    "- Open-ended (don‚Äôt map to a single document or source)\n",
    "\n",
    "üîç This makes retrieving from a single vector DB insufficient.\n",
    "\n",
    "Instead, we want an agent that can:\n",
    "\n",
    "- Decide what to fetch from where (retrieval planning)\n",
    "- Retrieve content from multiple tools (e.g., Wikipedia, PDFs, APIs, SQL)\n",
    "- Evaluate and merge that context\n",
    "- Produce a single human-like response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cb8854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders.youtube import YoutubeLoader\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "231639d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm=init_chat_model(\"groq:openai/gpt-oss-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5597d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_retriever(file_path):\n",
    "    docs = TextLoader(file_path, encoding=\"utf-8\").load()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    vs = FAISS.from_documents(chunks, HuggingFaceEmbeddings())\n",
    "    return vs.as_retriever()\n",
    "\n",
    "def load_youtube_retriever():\n",
    "    # Mocked YouTube transcript text\n",
    "    content = \"\"\"\n",
    "    This video explains how agentic AI systems rely on feedback loops, memory, and tool use.\n",
    "    It compares them to traditional pipeline-based LLMs. Temporal reasoning and autonomous tasking are emphasized.\n",
    "    \"\"\"\n",
    "    doc = Document(page_content=content, metadata={\"source\": \"youtube\"})\n",
    "    vectorstore = FAISS.from_documents([doc], HuggingFaceEmbeddings())\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "\n",
    "def wikipedia_search(query: str) -> str:\n",
    "    print(\"üåê Searching Wikipedia...\")\n",
    "    return WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper()).run(query)\n",
    "\n",
    "def arxiv_search(query: str) -> str:\n",
    "    print(\"üìÑ Searching ArXiv...\")\n",
    "    try:\n",
    "        results = ArxivLoader(query).load()\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in results[:2]) or \"No relevant papers found.\"\n",
    "    except Exception as e:\n",
    "        print(f\"ArXiv search failed: {e}\")\n",
    "        return \"ArXiv search unavailable. Mock result: Recent research on transformer agents includes work on reasoning, tool use, and autonomous planning.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "568f9ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_retriever = load_text_retriever(\"research_notes.txt\")\n",
    "youtube_retriever = load_youtube_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "867eea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### state\n",
    "class MultiSourceRAGState(BaseModel):\n",
    "    question: str\n",
    "    text_docs: List[Document] = []\n",
    "    yt_docs: List[Document] = []\n",
    "    wiki_context: str = \"\"\n",
    "    arxiv_context: str = \"\"\n",
    "    final_answer: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed9f025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieval Nodes\n",
    "def retrieve_text(state: MultiSourceRAGState) -> MultiSourceRAGState:\n",
    "    docs = text_retriever.invoke(state.question)\n",
    "    return state.model_copy(update={\"text_docs\": docs})\n",
    "\n",
    "def retrieve_yt(state: MultiSourceRAGState) -> MultiSourceRAGState:\n",
    "    docs = youtube_retriever.invoke(state.question)\n",
    "    return state.model_copy(update={\"yt_docs\": docs})\n",
    "\n",
    "def retrieve_wikipedia(state: MultiSourceRAGState) -> MultiSourceRAGState:\n",
    "    result = wikipedia_search(state.question)\n",
    "    return state.model_copy(update={\"wiki_context\": result})\n",
    "\n",
    "def retrieve_arxiv(state: MultiSourceRAGState) -> MultiSourceRAGState:\n",
    "    result = arxiv_search(state.question)\n",
    "    return state.model_copy(update={\"arxiv_context\": result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ed172aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## synthesize\n",
    "def synthesize_answer(state: MultiSourceRAGState) -> MultiSourceRAGState:\n",
    "    \n",
    "    context = \"\"\n",
    "\n",
    "    context += \"\\n\\n[Internal Docs]\\n\" + \"\\n\".join([doc.page_content for doc in state.text_docs])\n",
    "    context += \"\\n\\n[YouTube Transcript]\\n\" + \"\\n\".join([doc.page_content for doc in state.yt_docs])\n",
    "    context += \"\\n\\n[Wikipedia]\\n\" + state.wiki_context\n",
    "    context += \"\\n\\n[ArXiv]\\n\" + state.arxiv_context\n",
    "\n",
    "    prompt = f\"\"\"You have retrieved relevant context from multiple sources. Now synthesize a complete and coherent answer.\n",
    "\n",
    "Question: {state.question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Final Answer:\"\"\"\n",
    "\n",
    "    answer = llm.invoke(prompt).content.strip()\n",
    "    return state.model_copy(update={\"final_answer\": answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6201f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x000001F2FF92CF10>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder = StateGraph(MultiSourceRAGState)\n",
    "\n",
    "builder.add_node(\"retrieve_text\", retrieve_text)\n",
    "builder.add_node(\"retrieve_yt\", retrieve_yt)\n",
    "builder.add_node(\"retrieve_wiki\", retrieve_wikipedia)\n",
    "builder.add_node(\"retrieve_arxiv\", retrieve_arxiv)\n",
    "builder.add_node(\"synthesize\", synthesize_answer)\n",
    "\n",
    "builder.set_entry_point(\"retrieve_text\")\n",
    "builder.add_edge(\"retrieve_text\", \"retrieve_yt\")\n",
    "builder.add_edge(\"retrieve_yt\", \"retrieve_wiki\")\n",
    "builder.add_edge(\"retrieve_wiki\", \"retrieve_arxiv\")\n",
    "builder.add_edge(\"retrieve_arxiv\", \"synthesize\")\n",
    "builder.add_edge(\"synthesize\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0789c928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Searching Wikipedia...\n",
      "üìÑ Searching ArXiv...\n",
      "ArXiv search failed: PyMuPDF package not found, please install it with `pip install pymupdf`\n",
      "üìÑ Searching ArXiv...\n",
      "ArXiv search failed: PyMuPDF package not found, please install it with `pip install pymupdf`\n",
      "‚úÖ Final Answer:\n",
      "\n",
      "**Transformer agents** are a new class of AI systems that combine the raw representational power of large transformer‚Äëbased language models (LLMs) with the ability to *act* in an environment, *plan*, *remember*, and *use external tools*.  In practice they are usually built by taking a pre‚Äëtrained LLM, adding a small policy network (often the LLM itself), and training that policy to select actions that maximize a reward signal.  The actions can be anything that the system can ‚Äúcall‚Äù ‚Äì from writing a line of code, to querying a database, to sending an HTTP request, or simply generating the next text token.\n",
      "\n",
      "---\n",
      "\n",
      "## 1.  What a transformer agent actually looks like\n",
      "\n",
      "| Component | Typical implementation | Role |\n",
      "|-----------|------------------------|------|\n",
      "| **Core model** | GPT‚Äë4, GPT‚Äë5, Gemini‚Äë1, Claude‚Äë3, etc. | Generates text, predictions, and ‚Äúthoughts‚Äù. |\n",
      "| **Policy head** | A lightweight linear layer or the LLM itself | Decides which action to take next. |\n",
      "| **Action space** | Text generation, function calls, API requests, environment‚Äëspecific actions | Interface with the world. |\n",
      "| **Memory / state** | Short‚Äëterm memory buffer, external knowledge base, or a retrieval system | Keeps track of past interactions and relevant facts. |\n",
      "| **Reward / feedback** | Human‚Äëfeedback (RLHF), scripted metrics, or external signals | Drives learning of useful behavior. |\n",
      "| **Planner / router** | Optional hierarchical controller or a ‚Äúrouter‚Äù that chooses between fast vs. slow models | Enables efficient use of compute and knowledge. |\n",
      "\n",
      "The *agent* is thus a **closed loop**: observe ‚Üí reason ‚Üí act ‚Üí observe again.  It is no longer a static pipeline that just ‚Äúgenerates a response‚Äù but a dynamic system that can adapt its strategy over time.\n",
      "\n",
      "---\n",
      "\n",
      "## 2.  Evolution of transformer agents in recent research\n",
      "\n",
      "| Year | Milestone / Paper | Key Idea | Impact |\n",
      "|------|-------------------|----------|--------|\n",
      "| **2021** | *Chain‚Äëof‚ÄëThought Prompting* (Wei et‚ÄØal.) | Encourage the model to produce intermediate reasoning steps. | Showed that prompting can unlock higher‚Äëlevel reasoning without extra training. |\n",
      "| **2022** | *Retrieval‚ÄëAugmented Generation (RAG)* (Lewis et‚ÄØal.) | Combine LLM with a retriever to pull in up‚Äëto‚Äëdate facts. | Reduced hallucinations and improved factual accuracy. |\n",
      "| **2022** | *RLHF* (OpenAI, Anthropic) | Fine‚Äëtune LLM policies with human‚Äërated rewards. | Enabled safer, more aligned chatbots. |\n",
      "| **2023** | *Efficient / Sparse Transformers* (Swin Transformer, Longformer, BigBird, etc.) | Reduce quadratic attention cost. | Allowed longer contexts and larger models to be trained on commodity hardware. |\n",
      "| **2023** | *OpenAI GPT‚Äë4o* (multi‚Äëmodal) | Integrated vision & audio with language. | First step toward truly multimodal agents. |\n",
      "| **2024** | *GPT‚Äë5 with a Router* (OpenAI) | Automatic selection between a fast ‚Äúcompletion‚Äù model and a slow ‚Äúreasoning‚Äù model. | Demonstrated dynamic resource allocation for different tasks. |\n",
      "| **2024** | *Meta‚ÄëRL for Agent Planning* (Meta‚ÄëRL‚ÄëLLM) | Train a meta‚Äëpolicy that can adapt to new tasks quickly. | Accelerated few‚Äëshot learning for agents. |\n",
      "| **2024** | *Tool‚ÄëUse Agents* (e.g., BabyAGI, AutoGPT) | Agents that can call arbitrary APIs, write code, and debug. | Showed that LLMs can orchestrate complex workflows. |\n",
      "| **2025** | *Hierarchical Agent Frameworks* (e.g., Hierarchical RL‚ÄëLLM) | Decompose tasks into sub‚Äëgoals, each handled by a specialized sub‚Äëagent. | Improved scalability and explainability. |\n",
      "| **2025** | *Adaptive Retrieval & Knowledge Graphs* (RAG‚ÄëKG) | Combine retrieval with graph‚Äëbased reasoning. | Enhanced consistency and causal reasoning. |\n",
      "\n",
      "### Common themes across these developments\n",
      "\n",
      "| Theme | Why it matters | Representative work |\n",
      "|-------|----------------|---------------------|\n",
      "| **Reasoning & Planning** | Agents need to break down complex tasks. | Chain‚Äëof‚ÄëThought, GPT‚Äë5 router, hierarchical agents |\n",
      "| **External Knowledge** | LLMs alone hallucinate. | RAG, Retrieval‚Äëaugmented agents, knowledge‚Äëgraph integration |\n",
      "| **Alignment & Safety** | Human‚Äëlike output can be harmful. | RLHF, reward‚Äëshaping, safe‚ÄëRL |\n",
      "| **Efficiency** | Larger models are expensive. | Sparse Transformers, model distillation, router architecture |\n",
      "| **Multimodality** | Real‚Äëworld tasks involve vision/audio. | GPT‚Äë4o, multimodal LLM agents |\n",
      "| **Tool Use & Automation** | Agents must interact with software. | BabyAGI, AutoGPT, function‚Äëcalling APIs |\n",
      "| **Meta‚ÄëLearning & Rapid Adaptation** | New tasks arise constantly. | Meta‚ÄëRL‚ÄëLLM, few‚Äëshot agent fine‚Äëtuning |\n",
      "\n",
      "---\n",
      "\n",
      "## 3.  How the research pipeline is changing\n",
      "\n",
      "1. **From ‚Äúgenerate‚Äù to ‚Äúplan‚Äëact‚Äëlearn‚Äù**  \n",
      "   Early LLMs were purely generative.  Modern agents treat the LLM as a *policy* that can be conditioned on state, memory, and reward signals.\n",
      "\n",
      "2. **Integration of retrieval and memory**  \n",
      "   Retrieval‚Äëaugmented agents treat the world as an external memory that can be queried on demand.  This reduces hallucination and lets agents stay up‚Äëto‚Äëdate without re‚Äëtraining.\n",
      "\n",
      "3. **Hierarchical control**  \n",
      "   Instead of a flat policy, many agents now use a *router* or *hierarchical controller* to decide which sub‚Äëmodel or sub‚Äëpolicy to invoke, balancing speed and depth of reasoning.\n",
      "\n",
      "4. **Dynamic, compute‚Äëaware planning**  \n",
      "   GPT‚Äë5‚Äôs router is a concrete example of an agent that can decide, *during inference*, whether a fast completion or a slow reasoning pass is needed.  Future agents may make such decisions in real time, based on task complexity or user preference.\n",
      "\n",
      "5. **Tool‚Äëoriented APIs**  \n",
      "   Function‚Äëcalling and API‚Äëwrapping allow agents to treat third‚Äëparty services as primitive actions.  This turns the agent into a *software developer* that can write code, query databases, or control IoT devices.\n",
      "\n",
      "6. **Safety‚Äëfirst training**  \n",
      "   RLHF and safety‚Äëaligned reward functions are becoming standard.  Agents are no longer just ‚Äúsmart‚Äù but also *aligned* to human values and constraints.\n",
      "\n",
      "---\n",
      "\n",
      "## 4.  Take‚Äëaway: what transformer agents are today and where they‚Äôre headed\n",
      "\n",
      "- **Today**: Transformer agents are large LLMs (often GPT‚Äë4 or GPT‚Äë5‚Äëlike) wrapped in a policy loop that can use tools, retrieve knowledge, and plan.  They are used in chatbots, code‚Äëgenerators, virtual assistants, and autonomous problem‚Äësolvers.\n",
      "\n",
      "- **Soon**: We‚Äôll see agents that can *compose* multiple specialized sub‚Äëmodels, *learn* new tools on the fly, *plan* over longer horizons, and *self‚Äëcorrect* by querying external sources.  Efficiency will be tackled through sparse attention, model distillation, and compute‚Äëadaptive routing.\n",
      "\n",
      "- **Long‚Äëterm**: The ultimate goal is a **generalist agent** that can understand a task, decide which tools to use, orchestrate a multi‚Äëstep plan, and adapt its strategy based on feedback‚Äîall while remaining safe and aligned.\n",
      "\n",
      "In short, transformer agents are transforming the transformer from a static language model into a *dynamic, goal‚Äëdriven system* that can perceive, reason, and act in real‚Äëworld environments.  Research is rapidly pushing the boundary from ‚Äúchat‚Äù to ‚Äúagency‚Äù, and the next few years will bring even more sophisticated, efficient, and trustworthy agents.\n",
      "‚úÖ Final Answer:\n",
      "\n",
      "**Transformer agents** are a new class of AI systems that combine the raw representational power of large transformer‚Äëbased language models (LLMs) with the ability to *act* in an environment, *plan*, *remember*, and *use external tools*.  In practice they are usually built by taking a pre‚Äëtrained LLM, adding a small policy network (often the LLM itself), and training that policy to select actions that maximize a reward signal.  The actions can be anything that the system can ‚Äúcall‚Äù ‚Äì from writing a line of code, to querying a database, to sending an HTTP request, or simply generating the next text token.\n",
      "\n",
      "---\n",
      "\n",
      "## 1.  What a transformer agent actually looks like\n",
      "\n",
      "| Component | Typical implementation | Role |\n",
      "|-----------|------------------------|------|\n",
      "| **Core model** | GPT‚Äë4, GPT‚Äë5, Gemini‚Äë1, Claude‚Äë3, etc. | Generates text, predictions, and ‚Äúthoughts‚Äù. |\n",
      "| **Policy head** | A lightweight linear layer or the LLM itself | Decides which action to take next. |\n",
      "| **Action space** | Text generation, function calls, API requests, environment‚Äëspecific actions | Interface with the world. |\n",
      "| **Memory / state** | Short‚Äëterm memory buffer, external knowledge base, or a retrieval system | Keeps track of past interactions and relevant facts. |\n",
      "| **Reward / feedback** | Human‚Äëfeedback (RLHF), scripted metrics, or external signals | Drives learning of useful behavior. |\n",
      "| **Planner / router** | Optional hierarchical controller or a ‚Äúrouter‚Äù that chooses between fast vs. slow models | Enables efficient use of compute and knowledge. |\n",
      "\n",
      "The *agent* is thus a **closed loop**: observe ‚Üí reason ‚Üí act ‚Üí observe again.  It is no longer a static pipeline that just ‚Äúgenerates a response‚Äù but a dynamic system that can adapt its strategy over time.\n",
      "\n",
      "---\n",
      "\n",
      "## 2.  Evolution of transformer agents in recent research\n",
      "\n",
      "| Year | Milestone / Paper | Key Idea | Impact |\n",
      "|------|-------------------|----------|--------|\n",
      "| **2021** | *Chain‚Äëof‚ÄëThought Prompting* (Wei et‚ÄØal.) | Encourage the model to produce intermediate reasoning steps. | Showed that prompting can unlock higher‚Äëlevel reasoning without extra training. |\n",
      "| **2022** | *Retrieval‚ÄëAugmented Generation (RAG)* (Lewis et‚ÄØal.) | Combine LLM with a retriever to pull in up‚Äëto‚Äëdate facts. | Reduced hallucinations and improved factual accuracy. |\n",
      "| **2022** | *RLHF* (OpenAI, Anthropic) | Fine‚Äëtune LLM policies with human‚Äërated rewards. | Enabled safer, more aligned chatbots. |\n",
      "| **2023** | *Efficient / Sparse Transformers* (Swin Transformer, Longformer, BigBird, etc.) | Reduce quadratic attention cost. | Allowed longer contexts and larger models to be trained on commodity hardware. |\n",
      "| **2023** | *OpenAI GPT‚Äë4o* (multi‚Äëmodal) | Integrated vision & audio with language. | First step toward truly multimodal agents. |\n",
      "| **2024** | *GPT‚Äë5 with a Router* (OpenAI) | Automatic selection between a fast ‚Äúcompletion‚Äù model and a slow ‚Äúreasoning‚Äù model. | Demonstrated dynamic resource allocation for different tasks. |\n",
      "| **2024** | *Meta‚ÄëRL for Agent Planning* (Meta‚ÄëRL‚ÄëLLM) | Train a meta‚Äëpolicy that can adapt to new tasks quickly. | Accelerated few‚Äëshot learning for agents. |\n",
      "| **2024** | *Tool‚ÄëUse Agents* (e.g., BabyAGI, AutoGPT) | Agents that can call arbitrary APIs, write code, and debug. | Showed that LLMs can orchestrate complex workflows. |\n",
      "| **2025** | *Hierarchical Agent Frameworks* (e.g., Hierarchical RL‚ÄëLLM) | Decompose tasks into sub‚Äëgoals, each handled by a specialized sub‚Äëagent. | Improved scalability and explainability. |\n",
      "| **2025** | *Adaptive Retrieval & Knowledge Graphs* (RAG‚ÄëKG) | Combine retrieval with graph‚Äëbased reasoning. | Enhanced consistency and causal reasoning. |\n",
      "\n",
      "### Common themes across these developments\n",
      "\n",
      "| Theme | Why it matters | Representative work |\n",
      "|-------|----------------|---------------------|\n",
      "| **Reasoning & Planning** | Agents need to break down complex tasks. | Chain‚Äëof‚ÄëThought, GPT‚Äë5 router, hierarchical agents |\n",
      "| **External Knowledge** | LLMs alone hallucinate. | RAG, Retrieval‚Äëaugmented agents, knowledge‚Äëgraph integration |\n",
      "| **Alignment & Safety** | Human‚Äëlike output can be harmful. | RLHF, reward‚Äëshaping, safe‚ÄëRL |\n",
      "| **Efficiency** | Larger models are expensive. | Sparse Transformers, model distillation, router architecture |\n",
      "| **Multimodality** | Real‚Äëworld tasks involve vision/audio. | GPT‚Äë4o, multimodal LLM agents |\n",
      "| **Tool Use & Automation** | Agents must interact with software. | BabyAGI, AutoGPT, function‚Äëcalling APIs |\n",
      "| **Meta‚ÄëLearning & Rapid Adaptation** | New tasks arise constantly. | Meta‚ÄëRL‚ÄëLLM, few‚Äëshot agent fine‚Äëtuning |\n",
      "\n",
      "---\n",
      "\n",
      "## 3.  How the research pipeline is changing\n",
      "\n",
      "1. **From ‚Äúgenerate‚Äù to ‚Äúplan‚Äëact‚Äëlearn‚Äù**  \n",
      "   Early LLMs were purely generative.  Modern agents treat the LLM as a *policy* that can be conditioned on state, memory, and reward signals.\n",
      "\n",
      "2. **Integration of retrieval and memory**  \n",
      "   Retrieval‚Äëaugmented agents treat the world as an external memory that can be queried on demand.  This reduces hallucination and lets agents stay up‚Äëto‚Äëdate without re‚Äëtraining.\n",
      "\n",
      "3. **Hierarchical control**  \n",
      "   Instead of a flat policy, many agents now use a *router* or *hierarchical controller* to decide which sub‚Äëmodel or sub‚Äëpolicy to invoke, balancing speed and depth of reasoning.\n",
      "\n",
      "4. **Dynamic, compute‚Äëaware planning**  \n",
      "   GPT‚Äë5‚Äôs router is a concrete example of an agent that can decide, *during inference*, whether a fast completion or a slow reasoning pass is needed.  Future agents may make such decisions in real time, based on task complexity or user preference.\n",
      "\n",
      "5. **Tool‚Äëoriented APIs**  \n",
      "   Function‚Äëcalling and API‚Äëwrapping allow agents to treat third‚Äëparty services as primitive actions.  This turns the agent into a *software developer* that can write code, query databases, or control IoT devices.\n",
      "\n",
      "6. **Safety‚Äëfirst training**  \n",
      "   RLHF and safety‚Äëaligned reward functions are becoming standard.  Agents are no longer just ‚Äúsmart‚Äù but also *aligned* to human values and constraints.\n",
      "\n",
      "---\n",
      "\n",
      "## 4.  Take‚Äëaway: what transformer agents are today and where they‚Äôre headed\n",
      "\n",
      "- **Today**: Transformer agents are large LLMs (often GPT‚Äë4 or GPT‚Äë5‚Äëlike) wrapped in a policy loop that can use tools, retrieve knowledge, and plan.  They are used in chatbots, code‚Äëgenerators, virtual assistants, and autonomous problem‚Äësolvers.\n",
      "\n",
      "- **Soon**: We‚Äôll see agents that can *compose* multiple specialized sub‚Äëmodels, *learn* new tools on the fly, *plan* over longer horizons, and *self‚Äëcorrect* by querying external sources.  Efficiency will be tackled through sparse attention, model distillation, and compute‚Äëadaptive routing.\n",
      "\n",
      "- **Long‚Äëterm**: The ultimate goal is a **generalist agent** that can understand a task, decide which tools to use, orchestrate a multi‚Äëstep plan, and adapt its strategy based on feedback‚Äîall while remaining safe and aligned.\n",
      "\n",
      "In short, transformer agents are transforming the transformer from a static language model into a *dynamic, goal‚Äëdriven system* that can perceive, reason, and act in real‚Äëworld environments.  Research is rapidly pushing the boundary from ‚Äúchat‚Äù to ‚Äúagency‚Äù, and the next few years will bring even more sophisticated, efficient, and trustworthy agents.\n"
     ]
    }
   ],
   "source": [
    "question = \"What are transformer agents and how are they evolving in recent research?\"\n",
    "state = MultiSourceRAGState(question=question)\n",
    "result = graph.invoke(state)\n",
    "\n",
    "print(\"‚úÖ Final Answer:\\n\")\n",
    "print(result[\"final_answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88402b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are transformer agents and how are they evolving in recent research?',\n",
       " 'text_docs': [Document(id='6d4b5faa-d1a7-4406-aa70-2607541036ca', metadata={'source': 'research_notes.txt'}, page_content='Key papers/reading (suggested)\\n- \"Attention Is All You Need\"\\n- \"Scaling Laws for Neural Language Models\"\\n- \"Language Models are Few-Shot Learners\"\\n- \"Chain of Thought Prompting\"\\n- \"Retrieval-Augmented Generation (RAG)\"\\n- \"Reinforcement Learning from Human Feedback (RLHF)\"\\n- Survey papers on efficient / sparse Transformers'),\n",
       "  Document(id='c2250d1e-dd9b-447f-b40e-ee828f3f2c9a', metadata={'source': 'research_notes.txt'}, page_content='LLMs and Transformers ‚Äî Research Notes\\n=====================================\\n\\nOverview\\n- Large Language Models (LLMs) are deep neural networks trained on massive text corpora to model natural language via next-token or masked token prediction.\\n- Transformers replaced RNNs/CNNs with attention mechanisms, enabling parallelization and long-range dependencies.'),\n",
       "  Document(id='fcefaf5f-b173-4eeb-88f6-e17b8a73c26d', metadata={'source': 'research_notes.txt'}, page_content='Core Concepts\\n- Self-attention: computes pairwise token interactions via queries (Q), keys (K), values (V): Attention(Q,K,V)=softmax(QK^T / sqrt(dk))V.\\n- Multi-head attention: parallel attention heads capture different relationships; outputs concatenated then projected.\\n- Feed-Forward Network (FFN): per-position MLP with nonlinearity, typically two linear layers with GELU.\\n- Residual connections + LayerNorm stabilize deep training.'),\n",
       "  Document(id='56408049-77d0-468d-8431-8ea4fd7f17a6', metadata={'source': 'research_notes.txt'}, page_content='Interpretability & Analysis Methods\\n- Attention probing, probing classifiers, neuron/feature attribution, layer-wise behavior, representational similarity analysis.\\n- Behavioral evaluation: prompt suites, stress tests, and adversarial probes.')],\n",
       " 'yt_docs': [Document(id='dd2a4158-e30f-4b9f-a55e-817695eb2084', metadata={'source': 'youtube'}, page_content='\\n    This video explains how agentic AI systems rely on feedback loops, memory, and tool use.\\n    It compares them to traditional pipeline-based LLMs. Temporal reasoning and autonomous tasking are emphasized.\\n    ')],\n",
       " 'wiki_context': 'Page: Generative pre-trained transformer\\nSummary: A generative pre-trained transformer (GPT) is a type of large language model (LLM) that is widely used in generative AI chatbots. GPTs are based on a deep learning architecture called the transformer. They are pre-trained on large datasets of unlabeled content, and able to generate novel content.\\nOpenAI was the first to apply generative pre-training (GP) to the transformer architecture, introducing the GPT-1 model in 2018. The company has since released many bigger GPT models. The popular chatbot ChatGPT, released in late 2022 (using GPT-3.5), was followed by many competitor chatbots using their own \"GPT\" models to generate text, such as Gemini, DeepSeek or Claude.\\nGPTs are primarily used to generate text, but can be trained to generate other kinds of data. For example, GPT-4o can process and generate text, images and audio. To improve performance on complex tasks, some GPTs, such as OpenAI o3, spend more time analyzing the problem before generating an output, and are called reasoning models. In 2025, GPT-5 was released with a router that automatically selects whether to use a faster model or slower reasoning model based on task.\\n\\n\\n\\nPage: Large language model\\nSummary: A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation. The largest and most capable LLMs are generative pre-trained transformers (GPTs) and provide the core capabilities of chatbots such as ChatGPT, Gemini and Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained on.\\nThey consist of billions to trillions of parameters and operate as general-purpose sequence models, generating, summarizing, translating, and reasoning over text. LLMs represent a significant new technology in their ability to generalize across tasks with minimal task-specific supervision, enabling capabilities like conversational agents, code generation, knowledge retrieval, and automated reasoning that previously required bespoke systems.\\nLLMs evolved from earlier statistical and recurrent neural network approaches to language modeling. The transformer architecture, introduced in 2017, replaced recurrence with self-attention, allowing efficient parallelization, longer context handling, and scalable training on unprecedented data volumes. This innovation enabled models like GPT, BERT, and their successors, which demonstrated emergent behaviors at scale such as few-shot learning and compositional reasoning.\\nReinforcement learning, particularly policy gradient algorithms, has been adapted to fine-tune LLMs for desired behaviors beyond raw next-token prediction. Reinforcement learning from human feedback (RLHF) applies these methods to optimize a policy, the LLM\\'s output distribution, against reward signals derived from human or automated preference judgments. This has been critical for aligning model outputs with user expectations, improving factuality, reducing harmful responses, and enhancing task performance.\\nBenchmark evaluations for LLMs have evolved from narrow linguistic assessments toward comprehensive, multi-task evaluations measuring reasoning, factual accuracy, alignment, and safety. Hill climbing, iteratively optimizing models against benchmarks, has emerged as a dominant strategy, producing rapid incremental performance gains but raising concerns of overfitting to benchmarks rather than achieving genuine generalization or robust capability improvements.\\n\\n\\n\\nPage: Artificial intelligence\\nSummary: Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving',\n",
       " 'arxiv_context': 'ArXiv search unavailable. Mock result: Recent research on transformer agents includes work on reasoning, tool use, and autonomous planning.',\n",
       " 'final_answer': '**Transformer agents** are a new class of AI systems that combine the raw representational power of large transformer‚Äëbased language models (LLMs) with the ability to *act* in an environment, *plan*, *remember*, and *use external tools*.  In practice they are usually built by taking a pre‚Äëtrained LLM, adding a small policy network (often the LLM itself), and training that policy to select actions that maximize a reward signal.  The actions can be anything that the system can ‚Äúcall‚Äù ‚Äì from writing a line of code, to querying a database, to sending an HTTP request, or simply generating the next text token.\\n\\n---\\n\\n## 1.  What a transformer agent actually looks like\\n\\n| Component | Typical implementation | Role |\\n|-----------|------------------------|------|\\n| **Core model** | GPT‚Äë4, GPT‚Äë5, Gemini‚Äë1, Claude‚Äë3, etc. | Generates text, predictions, and ‚Äúthoughts‚Äù. |\\n| **Policy head** | A lightweight linear layer or the LLM itself | Decides which action to take next. |\\n| **Action space** | Text generation, function calls, API requests, environment‚Äëspecific actions | Interface with the world. |\\n| **Memory / state** | Short‚Äëterm memory buffer, external knowledge base, or a retrieval system | Keeps track of past interactions and relevant facts. |\\n| **Reward / feedback** | Human‚Äëfeedback (RLHF), scripted metrics, or external signals | Drives learning of useful behavior. |\\n| **Planner / router** | Optional hierarchical controller or a ‚Äúrouter‚Äù that chooses between fast vs. slow models | Enables efficient use of compute and knowledge. |\\n\\nThe *agent* is thus a **closed loop**: observe ‚Üí reason ‚Üí act ‚Üí observe again.  It is no longer a static pipeline that just ‚Äúgenerates a response‚Äù but a dynamic system that can adapt its strategy over time.\\n\\n---\\n\\n## 2.  Evolution of transformer agents in recent research\\n\\n| Year | Milestone / Paper | Key Idea | Impact |\\n|------|-------------------|----------|--------|\\n| **2021** | *Chain‚Äëof‚ÄëThought Prompting* (Wei et\\u202fal.) | Encourage the model to produce intermediate reasoning steps. | Showed that prompting can unlock higher‚Äëlevel reasoning without extra training. |\\n| **2022** | *Retrieval‚ÄëAugmented Generation (RAG)* (Lewis et\\u202fal.) | Combine LLM with a retriever to pull in up‚Äëto‚Äëdate facts. | Reduced hallucinations and improved factual accuracy. |\\n| **2022** | *RLHF* (OpenAI, Anthropic) | Fine‚Äëtune LLM policies with human‚Äërated rewards. | Enabled safer, more aligned chatbots. |\\n| **2023** | *Efficient / Sparse Transformers* (Swin Transformer, Longformer, BigBird, etc.) | Reduce quadratic attention cost. | Allowed longer contexts and larger models to be trained on commodity hardware. |\\n| **2023** | *OpenAI GPT‚Äë4o* (multi‚Äëmodal) | Integrated vision & audio with language. | First step toward truly multimodal agents. |\\n| **2024** | *GPT‚Äë5 with a Router* (OpenAI) | Automatic selection between a fast ‚Äúcompletion‚Äù model and a slow ‚Äúreasoning‚Äù model. | Demonstrated dynamic resource allocation for different tasks. |\\n| **2024** | *Meta‚ÄëRL for Agent Planning* (Meta‚ÄëRL‚ÄëLLM) | Train a meta‚Äëpolicy that can adapt to new tasks quickly. | Accelerated few‚Äëshot learning for agents. |\\n| **2024** | *Tool‚ÄëUse Agents* (e.g., BabyAGI, AutoGPT) | Agents that can call arbitrary APIs, write code, and debug. | Showed that LLMs can orchestrate complex workflows. |\\n| **2025** | *Hierarchical Agent Frameworks* (e.g., Hierarchical RL‚ÄëLLM) | Decompose tasks into sub‚Äëgoals, each handled by a specialized sub‚Äëagent. | Improved scalability and explainability. |\\n| **2025** | *Adaptive Retrieval & Knowledge Graphs* (RAG‚ÄëKG) | Combine retrieval with graph‚Äëbased reasoning. | Enhanced consistency and causal reasoning. |\\n\\n### Common themes across these developments\\n\\n| Theme | Why it matters | Representative work |\\n|-------|----------------|---------------------|\\n| **Reasoning & Planning** | Agents need to break down complex tasks. | Chain‚Äëof‚ÄëThought, GPT‚Äë5 router, hierarchical agents |\\n| **External Knowledge** | LLMs alone hallucinate. | RAG, Retrieval‚Äëaugmented agents, knowledge‚Äëgraph integration |\\n| **Alignment & Safety** | Human‚Äëlike output can be harmful. | RLHF, reward‚Äëshaping, safe‚ÄëRL |\\n| **Efficiency** | Larger models are expensive. | Sparse Transformers, model distillation, router architecture |\\n| **Multimodality** | Real‚Äëworld tasks involve vision/audio. | GPT‚Äë4o, multimodal LLM agents |\\n| **Tool Use & Automation** | Agents must interact with software. | BabyAGI, AutoGPT, function‚Äëcalling APIs |\\n| **Meta‚ÄëLearning & Rapid Adaptation** | New tasks arise constantly. | Meta‚ÄëRL‚ÄëLLM, few‚Äëshot agent fine‚Äëtuning |\\n\\n---\\n\\n## 3.  How the research pipeline is changing\\n\\n1. **From ‚Äúgenerate‚Äù to ‚Äúplan‚Äëact‚Äëlearn‚Äù**  \\n   Early LLMs were purely generative.  Modern agents treat the LLM as a *policy* that can be conditioned on state, memory, and reward signals.\\n\\n2. **Integration of retrieval and memory**  \\n   Retrieval‚Äëaugmented agents treat the world as an external memory that can be queried on demand.  This reduces hallucination and lets agents stay up‚Äëto‚Äëdate without re‚Äëtraining.\\n\\n3. **Hierarchical control**  \\n   Instead of a flat policy, many agents now use a *router* or *hierarchical controller* to decide which sub‚Äëmodel or sub‚Äëpolicy to invoke, balancing speed and depth of reasoning.\\n\\n4. **Dynamic, compute‚Äëaware planning**  \\n   GPT‚Äë5‚Äôs router is a concrete example of an agent that can decide, *during inference*, whether a fast completion or a slow reasoning pass is needed.  Future agents may make such decisions in real time, based on task complexity or user preference.\\n\\n5. **Tool‚Äëoriented APIs**  \\n   Function‚Äëcalling and API‚Äëwrapping allow agents to treat third‚Äëparty services as primitive actions.  This turns the agent into a *software developer* that can write code, query databases, or control IoT devices.\\n\\n6. **Safety‚Äëfirst training**  \\n   RLHF and safety‚Äëaligned reward functions are becoming standard.  Agents are no longer just ‚Äúsmart‚Äù but also *aligned* to human values and constraints.\\n\\n---\\n\\n## 4.  Take‚Äëaway: what transformer agents are today and where they‚Äôre headed\\n\\n- **Today**: Transformer agents are large LLMs (often GPT‚Äë4 or GPT‚Äë5‚Äëlike) wrapped in a policy loop that can use tools, retrieve knowledge, and plan.  They are used in chatbots, code‚Äëgenerators, virtual assistants, and autonomous problem‚Äësolvers.\\n\\n- **Soon**: We‚Äôll see agents that can *compose* multiple specialized sub‚Äëmodels, *learn* new tools on the fly, *plan* over longer horizons, and *self‚Äëcorrect* by querying external sources.  Efficiency will be tackled through sparse attention, model distillation, and compute‚Äëadaptive routing.\\n\\n- **Long‚Äëterm**: The ultimate goal is a **generalist agent** that can understand a task, decide which tools to use, orchestrate a multi‚Äëstep plan, and adapt its strategy based on feedback‚Äîall while remaining safe and aligned.\\n\\nIn short, transformer agents are transforming the transformer from a static language model into a *dynamic, goal‚Äëdriven system* that can perceive, reason, and act in real‚Äëworld environments.  Research is rapidly pushing the boundary from ‚Äúchat‚Äù to ‚Äúagency‚Äù, and the next few years will bring even more sophisticated, efficient, and trustworthy agents.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6425788d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AgenticAI-LangGraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
