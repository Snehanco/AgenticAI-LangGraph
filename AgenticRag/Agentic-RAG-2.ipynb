{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85395e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from typing import List, Annotated\n",
    "from pydantic import BaseModel\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "embeddings= HuggingFaceEmbeddings()\n",
    "llm = init_chat_model(\"groq:openai/gpt-oss-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36cde815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/overview', 'title': 'LangChain overview - Docs by LangChain', 'language': 'en'}, page_content='LangChain overview - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangChain v1.0Release notesMigration guideGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryUse in productionStudioTestDeployAgent Chat UIObservabilityOn this page Install Create an agent Core benefitsLangChain overviewCopy pageCopy pageLangChain v1.0 is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.\\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\\nLangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\\n\\u200b Install\\npipuvCopyAsk AIpip install -U langchain\\n# Requires Python 3.10+\\n\\n\\u200b Create an agent\\nCopyAsk AI# pip install -qU \"langchain[anthropic]\" to call the model\\n\\nfrom langchain.agents import create_agent\\n\\ndef get_weather(city: str) -> str:\\n    \"\"\"Get weather for a given city.\"\"\"\\n    return f\"It\\'s always sunny in {city}!\"\\n\\nagent = create_agent(\\n    model=\"claude-sonnet-4-5-20250929\",\\n    tools=[get_weather],\\n    system_prompt=\"You are a helpful assistant\",\\n)\\n\\n# Run the agent\\nagent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\\n)\\n\\n\\u200b Core benefits\\nStandard model interfaceDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.Learn moreEasy to use, highly flexible agentLangChain’s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.Learn moreBuilt on top of LangGraphLangChain’s agents are built on top of LangGraph. This allows us to take advantage of LangGraph’s durable execution, human-in-the-loop support, persistence, and more.Learn moreDebug with LangSmithGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.Learn more\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoWhat\\'s new in v1Next⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/quickstart', 'title': 'Quickstart - Docs by LangChain', 'language': 'en'}, page_content='Quickstart - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationGet startedQuickstartLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangChain v1.0Release notesMigration guideGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryUse in productionStudioTestDeployAgent Chat UIObservabilityOn this pageBuild a basic agentBuild a real-world agentGet startedQuickstartCopy pageCopy pageThis quickstart takes you from a simple setup to a fully functional AI agent in just a few minutes.\\n\\u200bBuild a basic agent\\nStart by creating a simple agent that can answer questions and call tools. The agent will use Claude Sonnet 4.5 as its language model, a basic weather function as a tool, and a simple prompt to guide its behavior.\\nFor this example, you will need to set up a Claude (Anthropic) account and get an API key. Then, set the ANTHROPIC_API_KEY environment variable in your terminal.\\nCopyAsk AIfrom langchain.agents import create_agent\\n\\ndef get_weather(city: str) -> str:\\n    \"\"\"Get weather for a given city.\"\"\"\\n    return f\"It\\'s always sunny in {city}!\"\\n\\nagent = create_agent(\\n    model=\"claude-sonnet-4-5-20250929\",\\n    tools=[get_weather],\\n    system_prompt=\"You are a helpful assistant\",\\n)\\n\\n# Run the agent\\nagent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\\n)\\n\\nTo learn how to trace your agent with LangSmith, see the LangSmith documentation.\\n\\u200bBuild a real-world agent\\nNext, build a practical weather forecasting agent that demonstrates key production concepts:\\n\\nDetailed system prompts for better agent behavior\\nCreate tools that integrate with external data\\nModel configuration for consistent responses\\nStructured output for predictable results\\nConversational memory for chat-like interactions\\nCreate and run the agent create a fully functional agent\\n\\nLet’s walk through each step:\\n1Define the system promptThe system prompt defines your agent’s role and behavior. Keep it specific and actionable:CopyAsk AISYSTEM_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns.\\n\\nYou have access to two tools:\\n\\n- get_weather_for_location: use this to get the weather for a specific location\\n- get_user_location: use this to get the user\\'s location\\n\\nIf a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.\"\"\"\\n2Create toolsTools let a model interact with external systems by calling functions you define.\\nTools can depend on runtime context and also interact with agent memory.Notice below how the get_user_location tool uses runtime context:CopyAsk AIfrom dataclasses import dataclass\\nfrom langchain.tools import tool, ToolRuntime\\n\\n@tool\\ndef get_weather_for_location(city: str) -> str:\\n    \"\"\"Get weather for a given city.\"\"\"\\n    return f\"It\\'s always sunny in {city}!\"\\n\\n@dataclass\\nclass Context:\\n    \"\"\"Custom runtime context schema.\"\"\"\\n    user_id: str\\n\\n@tool\\ndef get_user_location(runtime: ToolRuntime[Context]) -> str:\\n    \"\"\"Retrieve user information based on user ID.\"\"\"\\n    user_id = runtime.context.user_id\\n    return \"Florida\" if user_id == \"1\" else \"SF\"\\nTools should be well-documented: their name, description, and argument names become part of the model’s prompt.\\nLangChain’s @tool decorator adds metadata and enables runtime injection via the ToolRuntime parameter.3Configure your modelSet up your language model with the right parameters for your use case:CopyAsk AIfrom langchain.chat_models import init_chat_model\\n\\nmodel = init_chat_model(\\n    \"claude-sonnet-4-5-20250929\",\\n    temperature=0.5,\\n    timeout=10,\\n    max_tokens=1000\\n)\\n4Define response formatOptionally, define a structured response format if you need the agent responses to match\\na specific schema.CopyAsk AIfrom dataclasses import dataclass\\n\\n# We use a dataclass here, but Pydantic models are also supported.\\n@dataclass\\nclass ResponseFormat:\\n    \"\"\"Response schema for the agent.\"\"\"\\n    # A punny response (always required)\\n    punny_response: str\\n    # Any interesting information about the weather if available\\n    weather_conditions: str | None = None\\n5Add memoryAdd memory to your agent to maintain state across interactions. This allows\\nthe agent to remember previous conversations and context.CopyAsk AIfrom langgraph.checkpoint.memory import InMemorySaver\\n\\ncheckpointer = InMemorySaver()\\nIn production, use a persistent checkpointer that saves to a database.\\nSee Add and manage memory for more details.6Create and run the agentNow assemble your agent with all the components and run it!CopyAsk AIagent = create_agent(\\n    model=model,\\n    system_prompt=SYSTEM_PROMPT,\\n    tools=[get_user_location, get_weather_for_location],\\n    context_schema=Context,\\n    response_format=ResponseFormat,\\n    checkpointer=checkpointer\\n)\\n\\n# `thread_id` is a unique identifier for a given conversation.\\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\\n\\nresponse = agent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]},\\n    config=config,\\n    context=Context(user_id=\"1\")\\n)\\n\\nprint(response[\\'structured_response\\'])\\n# ResponseFormat(\\n#     punny_response=\"Florida is still having a \\'sun-derful\\' day! The sunshine is playing \\'ray-dio\\' hits all day long! I\\'d say it\\'s the perfect weather for some \\'solar-bration\\'! If you were hoping for rain, I\\'m afraid that idea is all \\'washed up\\' - the forecast remains \\'clear-ly\\' brilliant!\",\\n#     weather_conditions=\"It\\'s always sunny in Florida!\"\\n# )\\n\\n\\n# Note that we can continue the conversation using the same `thread_id`.\\nresponse = agent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\\n    config=config,\\n    context=Context(user_id=\"1\")\\n)\\n\\nprint(response[\\'structured_response\\'])\\n# ResponseFormat(\\n#     punny_response=\"You\\'re \\'thund-erfully\\' welcome! It\\'s always a \\'breeze\\' to help you stay \\'current\\' with the weather. I\\'m just \\'cloud\\'-ing around waiting to \\'shower\\' you with more forecasts whenever you need them. Have a \\'sun-sational\\' day in the Florida sunshine!\",\\n#     weather_conditions=None\\n# )\\n\\nShow Full example codeCopyAsk AIfrom dataclasses import dataclass\\n\\nfrom langchain.agents import create_agent\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain.tools import tool, ToolRuntime\\nfrom langgraph.checkpoint.memory import InMemorySaver\\n\\n\\n# Define system prompt\\nSYSTEM_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns.\\n\\nYou have access to two tools:\\n\\n- get_weather_for_location: use this to get the weather for a specific location\\n- get_user_location: use this to get the user\\'s location\\n\\nIf a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.\"\"\"\\n\\n# Define context schema\\n@dataclass\\nclass Context:\\n    \"\"\"Custom runtime context schema.\"\"\"\\n    user_id: str\\n\\n# Define tools\\n@tool\\ndef get_weather_for_location(city: str) -> str:\\n    \"\"\"Get weather for a given city.\"\"\"\\n    return f\"It\\'s always sunny in {city}!\"\\n\\n@tool\\ndef get_user_location(runtime: ToolRuntime[Context]) -> str:\\n    \"\"\"Retrieve user information based on user ID.\"\"\"\\n    user_id = runtime.context.user_id\\n    return \"Florida\" if user_id == \"1\" else \"SF\"\\n\\n# Configure model\\nmodel = init_chat_model(\\n    \"claude-sonnet-4-5-20250929\",\\n    temperature=0\\n)\\n\\n# Define response format\\n@dataclass\\nclass ResponseFormat:\\n    \"\"\"Response schema for the agent.\"\"\"\\n    # A punny response (always required)\\n    punny_response: str\\n    # Any interesting information about the weather if available\\n    weather_conditions: str | None = None\\n\\n# Set up memory\\ncheckpointer = InMemorySaver()\\n\\n# Create agent\\nagent = create_agent(\\n    model=model,\\n    system_prompt=SYSTEM_PROMPT,\\n    tools=[get_user_location, get_weather_for_location],\\n    context_schema=Context,\\n    response_format=ResponseFormat,\\n    checkpointer=checkpointer\\n)\\n\\n# Run agent\\n# `thread_id` is a unique identifier for a given conversation.\\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\\n\\nresponse = agent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]},\\n    config=config,\\n    context=Context(user_id=\"1\")\\n)\\n\\nprint(response[\\'structured_response\\'])\\n# ResponseFormat(\\n#     punny_response=\"Florida is still having a \\'sun-derful\\' day! The sunshine is playing \\'ray-dio\\' hits all day long! I\\'d say it\\'s the perfect weather for some \\'solar-bration\\'! If you were hoping for rain, I\\'m afraid that idea is all \\'washed up\\' - the forecast remains \\'clear-ly\\' brilliant!\",\\n#     weather_conditions=\"It\\'s always sunny in Florida!\"\\n# )\\n\\n\\n# Note that we can continue the conversation using the same `thread_id`.\\nresponse = agent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\\n    config=config,\\n    context=Context(user_id=\"1\")\\n)\\n\\nprint(response[\\'structured_response\\'])\\n# ResponseFormat(\\n#     punny_response=\"You\\'re \\'thund-erfully\\' welcome! It\\'s always a \\'breeze\\' to help you stay \\'current\\' with the weather. I\\'m just \\'cloud\\'-ing around waiting to \\'shower\\' you with more forecasts whenever you need them. Have a \\'sun-sational\\' day in the Florida sunshine!\",\\n#     weather_conditions=None\\n# )\\n\\nTo learn how to trace your agent with LangSmith, see the LangSmith documentation.\\nCongratulations! You now have an AI agent that can:\\n\\nUnderstand context and remember conversations\\nUse multiple tools intelligently\\nProvide structured responses in a consistent format\\nHandle user-specific information through context\\nMaintain conversation state across interactions\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoInstall LangChainPreviousPhilosophyNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/retrieval', 'title': 'Retrieval - Docs by LangChain', 'language': 'en'}, page_content='Retrieval - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationAdvanced usageRetrievalLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangChain v1.0Release notesMigration guideGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryUse in productionStudioTestDeployAgent Chat UIObservabilityOn this pageBuilding a knowledge baseFrom retrieval to RAGRetrieval PipelineBuilding BlocksRAG Architectures2-step RAGAgentic RAGHybrid RAGAdvanced usageRetrievalCopy pageCopy pageLarge language models (LLMs) are powerful, but they have two key limitations:\\n\\nFinite context — they can’t ingest entire corpora at once.\\nStatic knowledge — their training data is frozen at a point in time.\\n\\nRetrieval addresses these problems by fetching relevant external knowledge at query time. This is the foundation of Retrieval-Augmented Generation (RAG): enhancing an LLM’s answers with context-specific information.\\n\\u200bBuilding a knowledge base\\nA knowledge base is a repository of documents or structured data used during retrieval.\\nIf you need a custom knowledge base, you can use LangChain’s document loaders and vector stores to build one from your own data.\\nIf you already have a knowledge base (e.g., a SQL database, CRM, or internal documentation system), you do not need to rebuild it. You can:\\nConnect it as a tool for an agent in Agentic RAG.\\nQuery it and supply the retrieved content as context to the LLM (2-Step RAG).\\n\\nSee the following tutorial to build a searchable knowledge base and minimal RAG workflow:\\nTutorial: Semantic searchLearn how to create a searchable knowledge base from your own data using LangChain’s document loaders, embeddings, and vector stores.\\nIn this tutorial, you’ll build a search engine over a PDF, enabling retrieval of passages relevant to a query. You’ll also implement a minimal RAG workflow on top of this engine to see how external knowledge can be integrated into LLM reasoning.Learn more\\n\\u200bFrom retrieval to RAG\\nRetrieval allows LLMs to access relevant context at runtime. But most real-world applications go one step further: they integrate retrieval with generation to produce grounded, context-aware answers.\\nThis is the core idea behind Retrieval-Augmented Generation (RAG). The retrieval pipeline becomes a foundation for a broader system that combines search with generation.\\n\\u200bRetrieval Pipeline\\nA typical retrieval workflow looks like this:\\n\\nEach component is modular: you can swap loaders, splitters, embeddings, or vector stores without rewriting the app’s logic.\\n\\u200bBuilding Blocks\\nDocument loadersIngest data from external sources (Google Drive, Slack, Notion, etc.), returning standardized Document objects.Learn moreText splittersBreak large docs into smaller chunks that will be retrievable individually and fit within a model’s context window.Learn moreEmbedding modelsAn embedding model turns text into a vector of numbers so that texts with similar meaning land close together in that vector space.Learn moreVector storesSpecialized databases for storing and searching embeddings.Learn moreRetrieversA retriever is an interface that returns documents given an unstructured query.Learn more\\n\\u200bRAG Architectures\\nRAG can be implemented in multiple ways, depending on your system’s needs. We outline each type in the sections below.\\n\\nArchitectureDescriptionControlFlexibilityLatencyExample Use Case\\n2-Step RAGRetrieval always happens before generation. Simple and predictable✅ High❌ Low⚡ FastFAQs, documentation botsAgentic RAGAn LLM-powered agent decides when and how to retrieve during reasoning❌ Low✅ High⏳ VariableResearch assistants with access to multiple toolsHybridCombines characteristics of both approaches with validation steps⚖️ Medium⚖️ Medium⏳ VariableDomain-specific Q&A with quality validation\\n\\nLatency: Latency is generally more predictable in 2-Step RAG, as the maximum number of LLM calls is known and capped. This predictability assumes that LLM inference time is the dominant factor. However, real-world latency may also be affected by the performance of retrieval steps—such as API response times, network delays, or database queries—which can vary based on the tools and infrastructure in use.\\n\\u200b2-step RAG\\nIn 2-Step RAG, the retrieval step is always executed before the generation step. This architecture is straightforward and predictable, making it suitable for many applications where the retrieval of relevant documents is a clear prerequisite for generating an answer.\\n\\nTutorial: Retrieval-Augmented Generation (RAG)See how to build a Q&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.\\nThis tutorial walks through two approaches:\\nA RAG agent that runs searches with a flexible tool—great for general-purpose use.\\nA 2-step RAG chain that requires just one LLM call per query—fast and efficient for simpler tasks.\\nLearn more\\n\\u200bAgentic RAG\\nAgentic Retrieval-Augmented Generation (RAG) combines the strengths of Retrieval-Augmented Generation with agent-based reasoning. Instead of retrieving documents before answering, an agent (powered by an LLM) reasons step-by-step and decides when and how to retrieve information during the interaction.\\nThe only thing an agent needs to enable RAG behavior is access to one or more tools that can fetch external knowledge — such as documentation loaders, web APIs, or database queries.\\n\\nCopyAsk AIimport requests\\nfrom langchain.tools import tool\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain.agents import create_agent\\n\\n\\n@tool\\ndef fetch_url(url: str) -> str:\\n    \"\"\"Fetch text content from a URL\"\"\"\\n    response = requests.get(url, timeout=10.0)\\n    response.raise_for_status()\\n    return response.text\\n\\nsystem_prompt = \"\"\"\\\\\\nUse fetch_url when you need to fetch information from a web-page; quote relevant snippets.\\n\"\"\"\\n\\nagent = create_agent(\\n    model=\"claude-sonnet-4-5-20250929\",\\n    tools=[fetch_url], # A tool for retrieval\\n    system_prompt=system_prompt,\\n)\\n\\nShow Extended example: Agentic RAG for LangGraph\\'s llms.txtThis example implements an Agentic RAG system to assist users in querying LangGraph documentation. The agent begins by loading llms.txt, which lists available documentation URLs, and can then dynamically use a fetch_documentation tool to retrieve and process the relevant content based on the user’s question.CopyAsk AIimport requests\\nfrom langchain.agents import create_agent\\nfrom langchain.messages import HumanMessage\\nfrom langchain.tools import tool\\nfrom markdownify import markdownify\\n\\n\\nALLOWED_DOMAINS = [\"https://langchain-ai.github.io/\"]\\nLLMS_TXT = \\'https://langchain-ai.github.io/langgraph/llms.txt\\'\\n\\n\\n@tool\\ndef fetch_documentation(url: str) -> str:  \\n    \"\"\"Fetch and convert documentation from a URL\"\"\"\\n    if not any(url.startswith(domain) for domain in ALLOWED_DOMAINS):\\n        return (\\n            \"Error: URL not allowed. \"\\n            f\"Must start with one of: {\\', \\'.join(ALLOWED_DOMAINS)}\"\\n        )\\n    response = requests.get(url, timeout=10.0)\\n    response.raise_for_status()\\n    return markdownify(response.text)\\n\\n\\n# We will fetch the content of llms.txt, so this can\\n# be done ahead of time without requiring an LLM request.\\nllms_txt_content = requests.get(LLMS_TXT).text\\n\\n# System prompt for the agent\\nsystem_prompt = f\"\"\"\\nYou are an expert Python developer and technical assistant.\\nYour primary role is to help users with questions about LangGraph and related tools.\\n\\nInstructions:\\n\\n1. If a user asks a question you\\'re unsure about — or one that likely involves API usage,\\n   behavior, or configuration — you MUST use the `fetch_documentation` tool to consult the relevant docs.\\n2. When citing documentation, summarize clearly and include relevant context from the content.\\n3. Do not use any URLs outside of the allowed domain.\\n4. If a documentation fetch fails, tell the user and proceed with your best expert understanding.\\n\\nYou can access official documentation from the following approved sources:\\n\\n{llms_txt_content}\\n\\nYou MUST consult the documentation to get up to date documentation\\nbefore answering a user\\'s question about LangGraph.\\n\\nYour answers should be clear, concise, and technically accurate.\\n\"\"\"\\n\\ntools = [fetch_documentation]\\n\\nmodel = init_chat_model(\"claude-sonnet-4-0\", max_tokens=32_000)\\n\\nagent = create_agent(\\n    model=model,\\n    tools=tools,  \\n    system_prompt=system_prompt,  \\n    name=\"Agentic RAG\",\\n)\\n\\nresponse = agent.invoke({\\n    \\'messages\\': [\\n        HumanMessage(content=(\\n            \"Write a short example of a langgraph agent using the \"\\n            \"prebuilt create react agent. the agent should be able \"\\n            \"to look up stock pricing information.\"\\n        ))\\n    ]\\n})\\n\\nprint(response[\\'messages\\'][-1].content)\\n\\nTutorial: Retrieval-Augmented Generation (RAG)See how to build a Q&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.\\nThis tutorial walks through two approaches:\\nA RAG agent that runs searches with a flexible tool—great for general-purpose use.\\nA 2-step RAG chain that requires just one LLM call per query—fast and efficient for simpler tasks.\\nLearn more\\n\\u200bHybrid RAG\\nHybrid RAG combines characteristics of both 2-Step and Agentic RAG. It introduces intermediate steps such as query preprocessing, retrieval validation, and post-generation checks. These systems offer more flexibility than fixed pipelines while maintaining some control over execution.\\nTypical components include:\\n\\nQuery enhancement: Modify the input question to improve retrieval quality. This can involve rewriting unclear queries, generating multiple variations, or expanding queries with additional context.\\nRetrieval validation: Evaluate whether retrieved documents are relevant and sufficient. If not, the system may refine the query and retrieve again.\\nAnswer validation: Check the generated answer for accuracy, completeness, and alignment with source content. If needed, the system can regenerate or revise the answer.\\n\\nThe architecture often supports multiple iterations between these steps:\\n\\nThis architecture is suitable for:\\n\\nApplications with ambiguous or underspecified queries\\nSystems that require validation or quality control steps\\nWorkflows involving multiple sources or iterative refinement\\n\\nTutorial: Agentic RAG with Self-CorrectionAn example of Hybrid RAG that combines agentic reasoning with retrieval and self-correction.Learn more\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoMulti-agentPreviousLong-term memoryNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document Processing\n",
    "\n",
    "urls = [\n",
    "    \"https://docs.langchain.com/oss/python/langchain/overview\",\n",
    "    \"https://docs.langchain.com/oss/python/langchain/quickstart\",\n",
    "    \"https://docs.langchain.com/oss/python/langchain/retrieval\"\n",
    "]\n",
    "\n",
    "loaders = [WebBaseLoader(url) for url in urls]\n",
    "docs = []\n",
    "\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3d84485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e9eab42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='278244cd-6aad-4f6d-9ded-cd3e7e801e12', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/overview', 'title': 'LangChain overview - Docs by LangChain', 'language': 'en'}, page_content='LangChain overview - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangChain v1.0Release notesMigration guideGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryUse in productionStudioTestDeployAgent Chat UIObservabilityOn this page Install Create an agent Core benefitsLangChain overviewCopy pageCopy pageLangChain v1.0 is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please open an issue so we can improve. To'),\n",
       " Document(id='8f9441b2-329b-4773-9ab9-62f89b9124d9', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/overview', 'title': 'LangChain overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200b Core benefits\\nStandard model interfaceDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.Learn moreEasy to use, highly flexible agentLangChain’s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.Learn moreBuilt on top of LangGraphLangChain’s agents are built on top of LangGraph. This allows us to take advantage of LangGraph’s durable execution, human-in-the-loop support, persistence, and more.Learn moreDebug with LangSmithGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.Learn more'),\n",
       " Document(id='e0fbb7c5-f9bc-47e2-be73-314c47c24072', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/overview', 'title': 'LangChain overview - Docs by LangChain', 'language': 'en'}, page_content='LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\\nLangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\\n\\u200b Install\\npipuvCopyAsk AIpip install -U langchain\\n# Requires Python 3.10+'),\n",
       " Document(id='5f9bd6e2-517c-434c-a9c2-1c35724e4333', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/retrieval', 'title': 'Retrieval - Docs by LangChain', 'language': 'en'}, page_content='Retrieval - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationAdvanced usageRetrievalLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangChain v1.0Release notesMigration guideGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryUse in productionStudioTestDeployAgent Chat UIObservabilityOn this pageBuilding a knowledge baseFrom retrieval to RAGRetrieval PipelineBuilding BlocksRAG Architectures2-step RAGAgentic RAGHybrid RAGAdvanced usageRetrievalCopy pageCopy pageLarge language models (LLMs) are powerful, but they have two key limitations:')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is Langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d196e747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='retriever_vector_db_blog', description='Search and run information about Langchain', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x000001B0FA4C2700>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001B1FFC651D0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x000001B0D078FEC0>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001B1FFC651D0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool= create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retriever_vector_db_blog\",\n",
    "    \"Search and run information about Langchain\"\n",
    ")\n",
    "\n",
    "retriever_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c339e6f",
   "metadata": {},
   "source": [
    "Creating a separate vector store for storing Langgraph blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b86e9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/use-graph-api#map-reduce-and-the-send-api', 'title': 'Use the graph API - Docs by LangChain', 'language': 'en'}, page_content='Use the graph API - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationGraph APIUse the graph APILangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIGraph APIUse the graph APIFunctional APIRuntimeOn this pageSetupDefine and update stateDefine stateUpdate stateProcess state updates with reducersMessagesStateBypass reducers with OverwriteDefine input and output schemasPass private state between nodesUse Pydantic models for graph stateAdd runtime configurationAdd retry policiesAdd node cachingCreate a sequence of stepsCreate branchesRun graph nodes in parallelDefer node executionConditional branchingMap-Reduce and the Send APICreate and control loopsImpose a recursion limitAsyncCombine control flow and state updates with CommandNavigate to a node in a parent graphUse inside toolsVisualize your graphMermaidPNGLangGraph APIsGraph APIUse the graph APICopy pageCopy pageThis guide demonstrates the basics of LangGraph’s Graph API. It walks through state, as well as composing common graph structures such as sequences, branches, and loops. It also covers LangGraph’s control features, including the Send API for map-reduce workflows and the Command API for combining state updates with “hops” across nodes.\\n\\u200bSetup\\nInstall langgraph:\\npipuvCopyAsk AIpip install -U langgraph\\n\\nSet up LangSmith for better debuggingSign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started in the docs.\\n\\u200bDefine and update state\\nHere we show how to define and update state in LangGraph. We will demonstrate:\\n\\nHow to use state to define a graph’s schema\\nHow to use reducers to control how state updates are processed.\\n\\n\\u200bDefine state\\nState in LangGraph can be a TypedDict, Pydantic model, or dataclass. Below we will use TypedDict. See this section for detail on using Pydantic.\\nBy default, graphs will have the same input and output schema, and the state determines that schema. See this section for how to define distinct input and output schemas.\\nLet’s consider a simple example using messages. This represents a versatile formulation of state for many LLM applications. See our concepts page for more detail.\\nCopyAsk AIfrom langchain.messages import AnyMessage\\nfrom typing_extensions import TypedDict\\n\\nclass State(TypedDict):\\n    messages: list[AnyMessage]\\n    extra_field: int\\n\\nThis state tracks a list of message objects, as well as an extra integer field.\\n\\u200bUpdate state\\nLet’s build an example graph with a single node. Our node is just a Python function that reads our graph’s state and makes updates to it. The first argument to this function will always be the state:\\nCopyAsk AIfrom langchain.messages import AIMessage\\n\\ndef node(state: State):\\n    messages = state[\"messages\"]\\n    new_message = AIMessage(\"Hello!\")\\n    return {\"messages\": messages + [new_message], \"extra_field\": 10}\\n\\nThis node simply appends a message to our message list, and populates an extra field.\\nNodes should return updates to the state directly, instead of mutating the state.\\nLet’s next define a simple graph containing this node. We use StateGraph to define a graph that operates on this state. We then use add_node populate our graph.\\nCopyAsk AIfrom langgraph.graph import StateGraph\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(node)\\nbuilder.set_entry_point(\"node\")\\ngraph = builder.compile()\\n\\nLangGraph provides built-in utilities for visualizing your graph. Let’s inspect our graph. See this section for detail on visualization.\\nCopyAsk AIfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nIn this case, our graph just executes a single node. Let’s proceed with a simple invocation:\\nCopyAsk AIfrom langchain.messages import HumanMessage\\n\\nresult = graph.invoke({\"messages\": [HumanMessage(\"Hi\")]})\\nresult\\n\\nCopyAsk AI{\\'messages\\': [HumanMessage(content=\\'Hi\\'), AIMessage(content=\\'Hello!\\')], \\'extra_field\\': 10}\\n\\nNote that:\\n\\nWe kicked off invocation by updating a single key of the state.\\nWe receive the entire state in the invocation result.\\n\\nFor convenience, we frequently inspect the content of message objects via pretty-print:\\nCopyAsk AIfor message in result[\"messages\"]:\\n    message.pretty_print()\\n\\nCopyAsk AI================================ Human Message ================================\\n\\nHi\\n================================== Ai Message ==================================\\n\\nHello!\\n\\n\\u200bProcess state updates with reducers\\nEach key in the state can have its own independent reducer function, which controls how updates from nodes are applied. If no reducer function is explicitly specified then it is assumed that all updates to the key should override it.\\nFor TypedDict state schemas, we can define reducers by annotating the corresponding field of the state with a reducer function.\\nIn the earlier example, our node updated the \"messages\" key in the state by appending a message to it. Below, we add a reducer to this key, such that updates are automatically appended:\\nCopyAsk AIfrom typing_extensions import Annotated\\n\\ndef add(left, right):\\n    \"\"\"Can also import `add` from the `operator` built-in.\"\"\"\\n    return left + right\\n\\nclass State(TypedDict):\\n    messages: Annotated[list[AnyMessage], add]  \\n    extra_field: int\\n\\nNow our node can be simplified:\\nCopyAsk AIdef node(state: State):\\n    new_message = AIMessage(\"Hello!\")\\n    return {\"messages\": [new_message], \"extra_field\": 10}  \\n\\nCopyAsk AIfrom langgraph.graph import START\\n\\ngraph = StateGraph(State).add_node(node).add_edge(START, \"node\").compile()\\n\\nresult = graph.invoke({\"messages\": [HumanMessage(\"Hi\")]})\\n\\nfor message in result[\"messages\"]:\\n    message.pretty_print()\\n\\nCopyAsk AI================================ Human Message ================================\\n\\nHi\\n================================== Ai Message ==================================\\n\\nHello!\\n\\n\\u200bMessagesState\\nIn practice, there are additional considerations for updating lists of messages:\\n\\nWe may wish to update an existing message in the state.\\nWe may want to accept short-hands for message formats, such as OpenAI format.\\n\\nLangGraph includes a built-in reducer add_messages that handles these considerations:\\nCopyAsk AIfrom langgraph.graph.message import add_messages\\n\\nclass State(TypedDict):\\n    messages: Annotated[list[AnyMessage], add_messages]  \\n    extra_field: int\\n\\ndef node(state: State):\\n    new_message = AIMessage(\"Hello!\")\\n    return {\"messages\": [new_message], \"extra_field\": 10}\\n\\ngraph = StateGraph(State).add_node(node).set_entry_point(\"node\").compile()\\n\\nCopyAsk AIinput_message = {\"role\": \"user\", \"content\": \"Hi\"}  \\n\\nresult = graph.invoke({\"messages\": [input_message]})\\n\\nfor message in result[\"messages\"]:\\n    message.pretty_print()\\n\\nCopyAsk AI================================ Human Message ================================\\n\\nHi\\n================================== Ai Message ==================================\\n\\nHello!\\n\\nThis is a versatile representation of state for applications involving chat models. LangGraph includes a pre-built MessagesState for convenience, so that we can have:\\nCopyAsk AIfrom langgraph.graph import MessagesState\\n\\nclass State(MessagesState):\\n    extra_field: int\\n\\n\\u200bBypass reducers with Overwrite\\nIn some cases, you may want to bypass a reducer and directly overwrite a state value. LangGraph provides the Overwrite type for this purpose. When a node returns a value wrapped with Overwrite, the reducer is bypassed and the channel is set directly to that value.\\nThis is useful when you want to reset or replace accumulated state rather than merge it with existing values.\\nCopyAsk AIfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.types import Overwrite\\nfrom typing_extensions import Annotated, TypedDict\\nimport operator\\n\\nclass State(TypedDict):\\n    messages: Annotated[list, operator.add]\\n\\ndef add_message(state: State):\\n    return {\"messages\": [\"first message\"]}\\n\\ndef replace_messages(state: State):\\n    # Bypass the reducer and replace the entire messages list\\n    return {\"messages\": Overwrite([\"replacement message\"])}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(\"add_message\", add_message)\\nbuilder.add_node(\"replace_messages\", replace_messages)\\nbuilder.add_edge(START, \"add_message\")\\nbuilder.add_edge(\"add_message\", \"replace_messages\")\\nbuilder.add_edge(\"replace_messages\", END)\\n\\ngraph = builder.compile()\\n\\nresult = graph.invoke({\"messages\": [\"initial\"]})\\nprint(result[\"messages\"])\\n\\nCopyAsk AI[\\'replacement message\\']\\n\\nYou can also use JSON format with the special key \"__overwrite__\":\\nCopyAsk AIdef replace_messages(state: State):\\n    return {\"messages\": {\"__overwrite__\": [\"replacement message\"]}}\\n\\nWhen nodes execute in parallel, only one node can use Overwrite on the same state key in a given super-step. If multiple nodes attempt to overwrite the same key in the same super-step, an InvalidUpdateError will be raised.\\n\\u200bDefine input and output schemas\\nBy default, StateGraph operates with a single schema, and all nodes are expected to communicate using that schema. However, it’s also possible to define distinct input and output schemas for a graph.\\nWhen distinct schemas are specified, an internal schema will still be used for communication between nodes. The input schema ensures that the provided input matches the expected structure, while the output schema filters the internal data to return only the relevant information according to the defined output schema.\\nBelow, we’ll see how to define distinct input and output schema.\\nCopyAsk AIfrom langgraph.graph import StateGraph, START, END\\nfrom typing_extensions import TypedDict\\n\\n# Define the schema for the input\\nclass InputState(TypedDict):\\n    question: str\\n\\n# Define the schema for the output\\nclass OutputState(TypedDict):\\n    answer: str\\n\\n# Define the overall schema, combining both input and output\\nclass OverallState(InputState, OutputState):\\n    pass\\n\\n# Define the node that processes the input and generates an answer\\ndef answer_node(state: InputState):\\n    # Example answer and an extra key\\n    return {\"answer\": \"bye\", \"question\": state[\"question\"]}\\n\\n# Build the graph with input and output schemas specified\\nbuilder = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)\\nbuilder.add_node(answer_node)  # Add the answer node\\nbuilder.add_edge(START, \"answer_node\")  # Define the starting edge\\nbuilder.add_edge(\"answer_node\", END)  # Define the ending edge\\ngraph = builder.compile()  # Compile the graph\\n\\n# Invoke the graph with an input and print the result\\nprint(graph.invoke({\"question\": \"hi\"}))\\n\\nCopyAsk AI{\\'answer\\': \\'bye\\'}\\n\\nNotice that the output of invoke only includes the output schema.\\n\\u200bPass private state between nodes\\nIn some cases, you may want nodes to exchange information that is crucial for intermediate logic but doesn’t need to be part of the main schema of the graph. This private data is not relevant to the overall input/output of the graph and should only be shared between certain nodes.\\nBelow, we’ll create an example sequential graph consisting of three nodes (node_1, node_2 and node_3), where private data is passed between the first two steps (node_1 and node_2), while the third step (node_3) only has access to the public overall state.\\nCopyAsk AIfrom langgraph.graph import StateGraph, START, END\\nfrom typing_extensions import TypedDict\\n\\n# The overall state of the graph (this is the public state shared across nodes)\\nclass OverallState(TypedDict):\\n    a: str\\n\\n# Output from node_1 contains private data that is not part of the overall state\\nclass Node1Output(TypedDict):\\n    private_data: str\\n\\n# The private data is only shared between node_1 and node_2\\ndef node_1(state: OverallState) -> Node1Output:\\n    output = {\"private_data\": \"set by node_1\"}\\n    print(f\"Entered node `node_1`:\\\\n\\\\tInput: {state}.\\\\n\\\\tReturned: {output}\")\\n    return output\\n\\n# Node 2 input only requests the private data available after node_1\\nclass Node2Input(TypedDict):\\n    private_data: str\\n\\ndef node_2(state: Node2Input) -> OverallState:\\n    output = {\"a\": \"set by node_2\"}\\n    print(f\"Entered node `node_2`:\\\\n\\\\tInput: {state}.\\\\n\\\\tReturned: {output}\")\\n    return output\\n\\n# Node 3 only has access to the overall state (no access to private data from node_1)\\ndef node_3(state: OverallState) -> OverallState:\\n    output = {\"a\": \"set by node_3\"}\\n    print(f\"Entered node `node_3`:\\\\n\\\\tInput: {state}.\\\\n\\\\tReturned: {output}\")\\n    return output\\n\\n# Connect nodes in a sequence\\n# node_2 accepts private data from node_1, whereas\\n# node_3 does not see the private data.\\nbuilder = StateGraph(OverallState).add_sequence([node_1, node_2, node_3])\\nbuilder.add_edge(START, \"node_1\")\\ngraph = builder.compile()\\n\\n# Invoke the graph with the initial state\\nresponse = graph.invoke(\\n    {\\n        \"a\": \"set at start\",\\n    }\\n)\\n\\nprint()\\nprint(f\"Output of graph invocation: {response}\")\\n\\nCopyAsk AIEntered node `node_1`:\\n    Input: {\\'a\\': \\'set at start\\'}.\\n    Returned: {\\'private_data\\': \\'set by node_1\\'}\\nEntered node `node_2`:\\n    Input: {\\'private_data\\': \\'set by node_1\\'}.\\n    Returned: {\\'a\\': \\'set by node_2\\'}\\nEntered node `node_3`:\\n    Input: {\\'a\\': \\'set by node_2\\'}.\\n    Returned: {\\'a\\': \\'set by node_3\\'}\\n\\nOutput of graph invocation: {\\'a\\': \\'set by node_3\\'}\\n\\n\\u200bUse Pydantic models for graph state\\nA StateGraph accepts a state_schema argument on initialization that specifies the “shape” of the state that the nodes in the graph can access and update.\\nIn our examples, we typically use a python-native TypedDict or dataclass for state_schema, but state_schema can be any type.\\nHere, we’ll see how a Pydantic BaseModel can be used for state_schema to add run-time validation on inputs.\\nKnown Limitations\\nCurrently, the output of the graph will NOT be an instance of a pydantic model.\\nRun-time validation only occurs on inputs to the first node in the graph, not on subsequent nodes or outputs.\\nThe validation error trace from pydantic does not show which node the error arises in.\\nPydantic’s recursive validation can be slow. For performance-sensitive applications, you may want to consider using a dataclass instead.\\n\\nCopyAsk AIfrom langgraph.graph import StateGraph, START, END\\nfrom typing_extensions import TypedDict\\nfrom pydantic import BaseModel\\n\\n# The overall state of the graph (this is the public state shared across nodes)\\nclass OverallState(BaseModel):\\n    a: str\\n\\ndef node(state: OverallState):\\n    return {\"a\": \"goodbye\"}\\n\\n# Build the state graph\\nbuilder = StateGraph(OverallState)\\nbuilder.add_node(node)  # node_1 is the first node\\nbuilder.add_edge(START, \"node\")  # Start the graph with node_1\\nbuilder.add_edge(\"node\", END)  # End the graph after node_1\\ngraph = builder.compile()\\n\\n# Test the graph with a valid input\\ngraph.invoke({\"a\": \"hello\"})\\n\\nInvoke the graph with an invalid input\\nCopyAsk AItry:\\n    graph.invoke({\"a\": 123})  # Should be a string\\nexcept Exception as e:\\n    print(\"An exception was raised because `a` is an integer rather than a string.\")\\n    print(e)\\n\\nCopyAsk AIAn exception was raised because `a` is an integer rather than a string.\\n1 validation error for OverallState\\na\\n  Input should be a valid string [type=string_type, input_value=123, input_type=int]\\n    For further information visit https://errors.pydantic.dev/2.9/v/string_type\\n\\nSee below for additional features of Pydantic model state:\\nSerialization BehaviorWhen using Pydantic models as state schemas, it’s important to understand how serialization works, especially when:\\nPassing Pydantic objects as inputs\\nReceiving outputs from the graph\\nWorking with nested Pydantic models\\nLet’s see these behaviors in action.CopyAsk AIfrom langgraph.graph import StateGraph, START, END\\nfrom pydantic import BaseModel\\n\\nclass NestedModel(BaseModel):\\n    value: str\\n\\nclass ComplexState(BaseModel):\\n    text: str\\n    count: int\\n    nested: NestedModel\\n\\ndef process_node(state: ComplexState):\\n    # Node receives a validated Pydantic object\\n    print(f\"Input state type: {type(state)}\")\\n    print(f\"Nested type: {type(state.nested)}\")\\n    # Return a dictionary update\\n    return {\"text\": state.text + \" processed\", \"count\": state.count + 1}\\n\\n# Build the graph\\nbuilder = StateGraph(ComplexState)\\nbuilder.add_node(\"process\", process_node)\\nbuilder.add_edge(START, \"process\")\\nbuilder.add_edge(\"process\", END)\\ngraph = builder.compile()\\n\\n# Create a Pydantic instance for input\\ninput_state = ComplexState(text=\"hello\", count=0, nested=NestedModel(value=\"test\"))\\nprint(f\"Input object type: {type(input_state)}\")\\n\\n# Invoke graph with a Pydantic instance\\nresult = graph.invoke(input_state)\\nprint(f\"Output type: {type(result)}\")\\nprint(f\"Output content: {result}\")\\n\\n# Convert back to Pydantic model if needed\\noutput_model = ComplexState(**result)\\nprint(f\"Converted back to Pydantic: {type(output_model)}\")\\n\\nRuntime Type CoercionPydantic performs runtime type coercion for certain data types. This can be helpful but also lead to unexpected behavior if you’re not aware of it.CopyAsk AIfrom langgraph.graph import StateGraph, START, END\\nfrom pydantic import BaseModel\\n\\nclass CoercionExample(BaseModel):\\n    # Pydantic will coerce string numbers to integers\\n    number: int\\n    # Pydantic will parse string booleans to bool\\n    flag: bool\\n\\ndef inspect_node(state: CoercionExample):\\n    print(f\"number: {state.number} (type: {type(state.number)})\")\\n    print(f\"flag: {state.flag} (type: {type(state.flag)})\")\\n    return {}\\n\\nbuilder = StateGraph(CoercionExample)\\nbuilder.add_node(\"inspect\", inspect_node)\\nbuilder.add_edge(START, \"inspect\")\\nbuilder.add_edge(\"inspect\", END)\\ngraph = builder.compile()\\n\\n# Demonstrate coercion with string inputs that will be converted\\nresult = graph.invoke({\"number\": \"42\", \"flag\": \"true\"})\\n\\n# This would fail with a validation error\\ntry:\\n    graph.invoke({\"number\": \"not-a-number\", \"flag\": \"true\"})\\nexcept Exception as e:\\n    print(f\"\\\\nExpected validation error: {e}\")\\n\\nWorking with Message ModelsWhen working with LangChain message types in your state schema, there are important considerations for serialization. You should use AnyMessage (rather than BaseMessage) for proper serialization/deserialization when using message objects over the wire.CopyAsk AIfrom langgraph.graph import StateGraph, START, END\\nfrom pydantic import BaseModel\\nfrom langchain.messages import HumanMessage, AIMessage, AnyMessage\\nfrom typing import List\\n\\nclass ChatState(BaseModel):\\n    messages: List[AnyMessage]\\n    context: str\\n\\ndef add_message(state: ChatState):\\n    return {\"messages\": state.messages + [AIMessage(content=\"Hello there!\")]}\\n\\nbuilder = StateGraph(ChatState)\\nbuilder.add_node(\"add_message\", add_message)\\nbuilder.add_edge(START, \"add_message\")\\nbuilder.add_edge(\"add_message\", END)\\ngraph = builder.compile()\\n\\n# Create input with a message\\ninitial_state = ChatState(\\n    messages=[HumanMessage(content=\"Hi\")], context=\"Customer support chat\"\\n)\\n\\nresult = graph.invoke(initial_state)\\nprint(f\"Output: {result}\")\\n\\n# Convert back to Pydantic model to see message types\\noutput_model = ChatState(**result)\\nfor i, msg in enumerate(output_model.messages):\\n    print(f\"Message {i}: {type(msg).__name__} - {msg.content}\")\\n\\n\\u200bAdd runtime configuration\\nSometimes you want to be able to configure your graph when calling it. For example, you might want to be able to specify what LLM or system prompt to use at runtime, without polluting the graph state with these parameters.\\nTo add runtime configuration:\\n\\nSpecify a schema for your configuration\\nAdd the configuration to the function signature for nodes or conditional edges\\nPass the configuration into the graph.\\n\\nSee below for a simple example:\\nCopyAsk AIfrom langgraph.graph import END, StateGraph, START\\nfrom langgraph.runtime import Runtime\\nfrom typing_extensions import TypedDict\\n\\n# 1. Specify config schema\\nclass ContextSchema(TypedDict):\\n    my_runtime_value: str\\n\\n# 2. Define a graph that accesses the config in a node\\nclass State(TypedDict):\\n    my_state_value: str\\n\\ndef node(state: State, runtime: Runtime[ContextSchema]):  \\n    if runtime.context[\"my_runtime_value\"] == \"a\":  \\n        return {\"my_state_value\": 1}\\n    elif runtime.context[\"my_runtime_value\"] == \"b\":  \\n        return {\"my_state_value\": 2}\\n    else:\\n        raise ValueError(\"Unknown values.\")\\n\\nbuilder = StateGraph(State, context_schema=ContextSchema)  \\nbuilder.add_node(node)\\nbuilder.add_edge(START, \"node\")\\nbuilder.add_edge(\"node\", END)\\n\\ngraph = builder.compile()\\n\\n# 3. Pass in configuration at runtime:\\nprint(graph.invoke({}, context={\"my_runtime_value\": \"a\"}))  \\nprint(graph.invoke({}, context={\"my_runtime_value\": \"b\"}))  \\n\\nCopyAsk AI{\\'my_state_value\\': 1}\\n{\\'my_state_value\\': 2}\\n\\nExtended example: specifying LLM at runtimeBelow we demonstrate a practical example in which we configure what LLM to use at runtime. We will use both OpenAI and Anthropic models.CopyAsk AIfrom dataclasses import dataclass\\n\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import MessagesState, END, StateGraph, START\\nfrom langgraph.runtime import Runtime\\nfrom typing_extensions import TypedDict\\n\\n@dataclass\\nclass ContextSchema:\\n    model_provider: str = \"anthropic\"\\n\\nMODELS = {\\n    \"anthropic\": init_chat_model(\"claude-haiku-4-5-20251001\"),\\n    \"openai\": init_chat_model(\"gpt-4.1-mini\"),\\n}\\n\\ndef call_model(state: MessagesState, runtime: Runtime[ContextSchema]):\\n    model = MODELS[runtime.context.model_provider]\\n    response = model.invoke(state[\"messages\"])\\n    return {\"messages\": [response]}\\n\\nbuilder = StateGraph(MessagesState, context_schema=ContextSchema)\\nbuilder.add_node(\"model\", call_model)\\nbuilder.add_edge(START, \"model\")\\nbuilder.add_edge(\"model\", END)\\n\\ngraph = builder.compile()\\n\\n# Usage\\ninput_message = {\"role\": \"user\", \"content\": \"hi\"}\\n# With no configuration, uses default (Anthropic)\\nresponse_1 = graph.invoke({\"messages\": [input_message]}, context=ContextSchema())[\"messages\"][-1]\\n# Or, can set OpenAI\\nresponse_2 = graph.invoke({\"messages\": [input_message]}, context={\"model_provider\": \"openai\"})[\"messages\"][-1]\\n\\nprint(response_1.response_metadata[\"model_name\"])\\nprint(response_2.response_metadata[\"model_name\"])\\nCopyAsk AIclaude-haiku-4-5-20251001\\ngpt-4.1-mini-2025-04-14\\n\\nExtended example: specifying model and system message at runtimeBelow we demonstrate a practical example in which we configure two parameters: the LLM and system message to use at runtime.CopyAsk AIfrom dataclasses import dataclass\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain.messages import SystemMessage\\nfrom langgraph.graph import END, MessagesState, StateGraph, START\\nfrom langgraph.runtime import Runtime\\nfrom typing_extensions import TypedDict\\n\\n@dataclass\\nclass ContextSchema:\\n    model_provider: str = \"anthropic\"\\n    system_message: str | None = None\\n\\nMODELS = {\\n    \"anthropic\": init_chat_model(\"claude-haiku-4-5-20251001\"),\\n    \"openai\": init_chat_model(\"gpt-4.1-mini\"),\\n}\\n\\ndef call_model(state: MessagesState, runtime: Runtime[ContextSchema]):\\n    model = MODELS[runtime.context.model_provider]\\n    messages = state[\"messages\"]\\n    if (system_message := runtime.context.system_message):\\n        messages = [SystemMessage(system_message)] + messages\\n    response = model.invoke(messages)\\n    return {\"messages\": [response]}\\n\\nbuilder = StateGraph(MessagesState, context_schema=ContextSchema)\\nbuilder.add_node(\"model\", call_model)\\nbuilder.add_edge(START, \"model\")\\nbuilder.add_edge(\"model\", END)\\n\\ngraph = builder.compile()\\n\\n# Usage\\ninput_message = {\"role\": \"user\", \"content\": \"hi\"}\\nresponse = graph.invoke({\"messages\": [input_message]}, context={\"model_provider\": \"openai\", \"system_message\": \"Respond in Italian.\"})\\nfor message in response[\"messages\"]:\\n    message.pretty_print()\\nCopyAsk AI================================ Human Message ================================\\n\\nhi\\n================================== Ai Message ==================================\\n\\nCiao! Come posso aiutarti oggi?\\n\\n\\u200bAdd retry policies\\nThere are many use cases where you may wish for your node to have a custom retry policy, for example if you are calling an API, querying a database, or calling an LLM, etc. LangGraph lets you add retry policies to nodes.\\nTo configure a retry policy, pass the retry_policy parameter to the add_node. The retry_policy parameter takes in a RetryPolicy named tuple object. Below we instantiate a RetryPolicy object with the default parameters and associate it with a node:\\nCopyAsk AIfrom langgraph.types import RetryPolicy\\n\\nbuilder.add_node(\\n    \"node_name\",\\n    node_function,\\n    retry_policy=RetryPolicy(),\\n)\\n\\nBy default, the retry_on parameter uses the default_retry_on function, which retries on any exception except for the following:\\n\\nValueError\\nTypeError\\nArithmeticError\\nImportError\\nLookupError\\nNameError\\nSyntaxError\\nRuntimeError\\nReferenceError\\nStopIteration\\nStopAsyncIteration\\nOSError\\n\\nIn addition, for exceptions from popular http request libraries such as requests and httpx it only retries on 5xx status codes.\\nExtended example: customizing retry policiesConsider an example in which we are reading from a SQL database. Below we pass two different retry policies to nodes:CopyAsk AIimport sqlite3\\nfrom typing_extensions import TypedDict\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import END, MessagesState, StateGraph, START\\nfrom langgraph.types import RetryPolicy\\nfrom langchain_community.utilities import SQLDatabase\\nfrom langchain.messages import AIMessage\\n\\ndb = SQLDatabase.from_uri(\"sqlite:///:memory:\")\\nmodel = init_chat_model(\"claude-haiku-4-5-20251001\")\\n\\ndef query_database(state: MessagesState):\\n    query_result = db.run(\"SELECT * FROM Artist LIMIT 10;\")\\n    return {\"messages\": [AIMessage(content=query_result)]}\\n\\ndef call_model(state: MessagesState):\\n    response = model.invoke(state[\"messages\"])\\n    return {\"messages\": [response]}\\n\\n# Define a new graph\\nbuilder = StateGraph(MessagesState)\\nbuilder.add_node(\\n    \"query_database\",\\n    query_database,\\n    retry_policy=RetryPolicy(retry_on=sqlite3.OperationalError),\\n)\\nbuilder.add_node(\"model\", call_model, retry_policy=RetryPolicy(max_attempts=5))\\nbuilder.add_edge(START, \"model\")\\nbuilder.add_edge(\"model\", \"query_database\")\\nbuilder.add_edge(\"query_database\", END)\\ngraph = builder.compile()\\n\\n\\u200bAdd node caching\\nNode caching is useful in cases where you want to avoid repeating operations, like when doing something expensive (either in terms of time or cost). LangGraph lets you add individualized caching policies to nodes in a graph.\\nTo configure a cache policy, pass the cache_policy parameter to the add_node function. In the following example, a CachePolicy object is instantiated with a time to live of 120 seconds and the default key_func generator. Then it is associated with a node:\\nCopyAsk AIfrom langgraph.types import CachePolicy\\n\\nbuilder.add_node(\\n    \"node_name\",\\n    node_function,\\n    cache_policy=CachePolicy(ttl=120),\\n)\\n\\nThen, to enable node-level caching for a graph, set the cache argument when compiling the graph. The example below uses InMemoryCache to set up a graph with in-memory cache, but SqliteCache is also available.\\nCopyAsk AIfrom langgraph.cache.memory import InMemoryCache\\n\\ngraph = builder.compile(cache=InMemoryCache())\\n\\n\\u200bCreate a sequence of steps\\nPrerequisites\\nThis guide assumes familiarity with the above section on state.\\nHere we demonstrate how to construct a simple sequence of steps. We will show:\\n\\nHow to build a sequential graph\\nBuilt-in short-hand for constructing similar graphs.\\n\\nTo add a sequence of nodes, we use the add_node and add_edge methods of our graph:\\nCopyAsk AIfrom langgraph.graph import START, StateGraph\\n\\nbuilder = StateGraph(State)\\n\\n# Add nodes\\nbuilder.add_node(step_1)\\nbuilder.add_node(step_2)\\nbuilder.add_node(step_3)\\n\\n# Add edges\\nbuilder.add_edge(START, \"step_1\")\\nbuilder.add_edge(\"step_1\", \"step_2\")\\nbuilder.add_edge(\"step_2\", \"step_3\")\\n\\nWe can also use the built-in shorthand .add_sequence:\\nCopyAsk AIbuilder = StateGraph(State).add_sequence([step_1, step_2, step_3])\\nbuilder.add_edge(START, \"step_1\")\\n\\nWhy split application steps into a sequence with LangGraph?LangGraph makes it easy to add an underlying persistence layer to your application.\\nThis allows state to be checkpointed in between the execution of nodes, so your LangGraph nodes govern:\\nHow state updates are checkpointed\\nHow interruptions are resumed in human-in-the-loop workflows\\nHow we can “rewind” and branch-off executions using LangGraph’s time travel features\\nThey also determine how execution steps are streamed, and how your application is visualized and debugged using Studio.Let’s demonstrate an end-to-end example. We will create a sequence of three steps:\\nPopulate a value in a key of the state\\nUpdate the same value\\nPopulate a different value\\nLet’s first define our state. This governs the schema of the graph, and can also specify how to apply updates. See this section for more detail.In our case, we will just keep track of two values:CopyAsk AIfrom typing_extensions import TypedDict\\n\\nclass State(TypedDict):\\n    value_1: str\\n    value_2: int\\nOur nodes are just Python functions that read our graph’s state and make updates to it. The first argument to this function will always be the state:CopyAsk AIdef step_1(state: State):\\n    return {\"value_1\": \"a\"}\\n\\ndef step_2(state: State):\\n    current_value_1 = state[\"value_1\"]\\n    return {\"value_1\": f\"{current_value_1} b\"}\\n\\ndef step_3(state: State):\\n    return {\"value_2\": 10}\\nNote that when issuing updates to the state, each node can just specify the value of the key it wishes to update.By default, this will overwrite the value of the corresponding key. You can also use reducers to control how updates are processed— for example, you can append successive updates to a key instead. See this section for more detail.Finally, we define the graph. We use StateGraph to define a graph that operates on this state.We will then use add_node and add_edge to populate our graph and define its control flow.CopyAsk AIfrom langgraph.graph import START, StateGraph\\n\\nbuilder = StateGraph(State)\\n\\n# Add nodes\\nbuilder.add_node(step_1)\\nbuilder.add_node(step_2)\\nbuilder.add_node(step_3)\\n\\n# Add edges\\nbuilder.add_edge(START, \"step_1\")\\nbuilder.add_edge(\"step_1\", \"step_2\")\\nbuilder.add_edge(\"step_2\", \"step_3\")\\nSpecifying custom names\\nYou can specify custom names for nodes using add_node:CopyAsk AIbuilder.add_node(\"my_node\", step_1)\\nNote that:\\nadd_edge takes the names of nodes, which for functions defaults to node.__name__.\\nWe must specify the entry point of the graph. For this we add an edge with the START node.\\nThe graph halts when there are no more nodes to execute.\\nWe next compile our graph. This provides a few basic checks on the structure of the graph (e.g., identifying orphaned nodes). If we were adding persistence to our application via a checkpointer, it would also be passed in here.CopyAsk AIgraph = builder.compile()\\nLangGraph provides built-in utilities for visualizing your graph. Let’s inspect our sequence. See this guide for detail on visualization.CopyAsk AIfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\nLet’s proceed with a simple invocation:CopyAsk AIgraph.invoke({\"value_1\": \"c\"})\\nCopyAsk AI{\\'value_1\\': \\'a b\\', \\'value_2\\': 10}\\nNote that:\\nWe kicked off invocation by providing a value for a single state key. We must always provide a value for at least one key.\\nThe value we passed in was overwritten by the first node.\\nThe second node updated the value.\\nThe third node populated a different value.\\nBuilt-in shorthand\\nlanggraph>=0.2.46 includes a built-in short-hand add_sequence for adding node sequences. You can compile the same graph as follows:CopyAsk AIbuilder = StateGraph(State).add_sequence([step_1, step_2, step_3])  \\nbuilder.add_edge(START, \"step_1\")\\n\\ngraph = builder.compile()\\n\\ngraph.invoke({\"value_1\": \"c\"})\\n\\n\\u200bCreate branches\\nParallel execution of nodes is essential to speed up overall graph operation. LangGraph offers native support for parallel execution of nodes, which can significantly enhance the performance of graph-based workflows. This parallelization is achieved through fan-out and fan-in mechanisms, utilizing both standard edges and conditional_edges. Below are some examples showing how to add create branching dataflows that work for you.\\n\\u200bRun graph nodes in parallel\\nIn this example, we fan out from Node A to B and C and then fan in to D. With our state, we specify the reducer add operation. This will combine or accumulate values for the specific key in the State, rather than simply overwriting the existing value. For lists, this means concatenating the new list with the existing list. See the above section on state reducers for more detail on updating state with reducers.\\nCopyAsk AIimport operator\\nfrom typing import Annotated, Any\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Adding \"A\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Adding \"B\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef c(state: State):\\n    print(f\\'Adding \"C\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\ndef d(state: State):\\n    print(f\\'Adding \"D\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"D\"]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(c)\\nbuilder.add_node(d)\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_edge(\"a\", \"b\")\\nbuilder.add_edge(\"a\", \"c\")\\nbuilder.add_edge(\"b\", \"d\")\\nbuilder.add_edge(\"c\", \"d\")\\nbuilder.add_edge(\"d\", END)\\ngraph = builder.compile()\\n\\nCopyAsk AIfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nWith the reducer, you can see that the values added in each node are accumulated.\\nCopyAsk AIgraph.invoke({\"aggregate\": []}, {\"configurable\": {\"thread_id\": \"foo\"}})\\n\\nCopyAsk AIAdding \"A\" to []\\nAdding \"B\" to [\\'A\\']\\nAdding \"C\" to [\\'A\\']\\nAdding \"D\" to [\\'A\\', \\'B\\', \\'C\\']\\n\\nIn the above example, nodes \"b\" and \"c\" are executed concurrently in the same superstep. Because they are in the same step, node \"d\" executes after both \"b\" and \"c\" are finished.Importantly, updates from a parallel superstep may not be ordered consistently. If you need a consistent, predetermined ordering of updates from a parallel superstep, you should write the outputs to a separate field in the state together with a value with which to order them.\\nException handling?LangGraph executes nodes within supersteps, meaning that while parallel branches are executed in parallel, the entire superstep is transactional. If any of these branches raises an exception, none of the updates are applied to the state (the entire superstep errors).Importantly, when using a checkpointer, results from successful nodes within a superstep are saved, and don’t repeat when resumed.If you have error-prone (perhaps want to handle flakey API calls), LangGraph provides two ways to address this:\\nYou can write regular python code within your node to catch and handle exceptions.\\nYou can set a retry_policy to direct the graph to retry nodes that raise certain types of exceptions. Only failing branches are retried, so you needn’t worry about performing redundant work.\\nTogether, these let you perform parallel execution and fully control exception handling.\\nSet max concurrency\\nYou can control the maximum number of concurrent tasks by setting max_concurrency in the configuration when invoking the graph.CopyAsk AIgraph.invoke({\"value_1\": \"c\"}, {\"configurable\": {\"max_concurrency\": 10}})\\n\\n\\u200bDefer node execution\\nDeferring node execution is useful when you want to delay the execution of a node until all other pending tasks are completed. This is particularly relevant when branches have different lengths, which is common in workflows like map-reduce flows.\\nThe above example showed how to fan-out and fan-in when each path was only one step. But what if one branch had more than one step? Let’s add a node \"b_2\" in the \"b\" branch:\\nCopyAsk AIimport operator\\nfrom typing import Annotated, Any\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Adding \"A\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Adding \"B\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef b_2(state: State):\\n    print(f\\'Adding \"B_2\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B_2\"]}\\n\\ndef c(state: State):\\n    print(f\\'Adding \"C\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\ndef d(state: State):\\n    print(f\\'Adding \"D\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"D\"]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(b_2)\\nbuilder.add_node(c)\\nbuilder.add_node(d, defer=True)  \\nbuilder.add_edge(START, \"a\")\\nbuilder.add_edge(\"a\", \"b\")\\nbuilder.add_edge(\"a\", \"c\")\\nbuilder.add_edge(\"b\", \"b_2\")\\nbuilder.add_edge(\"b_2\", \"d\")\\nbuilder.add_edge(\"c\", \"d\")\\nbuilder.add_edge(\"d\", END)\\ngraph = builder.compile()\\n\\nCopyAsk AIfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nCopyAsk AIgraph.invoke({\"aggregate\": []})\\n\\nCopyAsk AIAdding \"A\" to []\\nAdding \"B\" to [\\'A\\']\\nAdding \"C\" to [\\'A\\']\\nAdding \"B_2\" to [\\'A\\', \\'B\\', \\'C\\']\\nAdding \"D\" to [\\'A\\', \\'B\\', \\'C\\', \\'B_2\\']\\n\\nIn the above example, nodes \"b\" and \"c\" are executed concurrently in the same superstep. We set defer=True on node d so it will not execute until all pending tasks are finished. In this case, this means that \"d\" waits to execute until the entire \"b\" branch is finished.\\n\\u200bConditional branching\\nIf your fan-out should vary at runtime based on the state, you can use add_conditional_edges to select one or more paths using the graph state. See example below, where node a generates a state update that determines the following node.\\nCopyAsk AIimport operator\\nfrom typing import Annotated, Literal, Sequence\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    aggregate: Annotated[list, operator.add]\\n    # Add a key to the state. We will set this key to determine\\n    # how we branch.\\n    which: str\\n\\ndef a(state: State):\\n    print(f\\'Adding \"A\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"], \"which\": \"c\"}  \\n\\ndef b(state: State):\\n    print(f\\'Adding \"B\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef c(state: State):\\n    print(f\\'Adding \"C\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(c)\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_edge(\"b\", END)\\nbuilder.add_edge(\"c\", END)\\n\\ndef conditional_edge(state: State) -> Literal[\"b\", \"c\"]:\\n    # Fill in arbitrary logic here that uses the state\\n    # to determine the next node\\n    return state[\"which\"]\\n\\nbuilder.add_conditional_edges(\"a\", conditional_edge)  \\n\\ngraph = builder.compile()\\n\\nCopyAsk AIfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nCopyAsk AIresult = graph.invoke({\"aggregate\": []})\\nprint(result)\\n\\nCopyAsk AIAdding \"A\" to []\\nAdding \"C\" to [\\'A\\']\\n{\\'aggregate\\': [\\'A\\', \\'C\\'], \\'which\\': \\'c\\'}\\n\\nYour conditional edges can route to multiple destination nodes. For example:CopyAsk AIdef route_bc_or_cd(state: State) -> Sequence[str]:\\n    if state[\"which\"] == \"cd\":\\n        return [\"c\", \"d\"]\\n    return [\"b\", \"c\"]\\n\\n\\u200bMap-Reduce and the Send API\\nLangGraph supports map-reduce and other advanced branching patterns using the Send API. Here is an example of how to use it:\\nCopyAsk AIfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.types import Send\\nfrom typing_extensions import TypedDict, Annotated\\nimport operator\\n\\nclass OverallState(TypedDict):\\n    topic: str\\n    subjects: list[str]\\n    jokes: Annotated[list[str], operator.add]\\n    best_selected_joke: str\\n\\ndef generate_topics(state: OverallState):\\n    return {\"subjects\": [\"lions\", \"elephants\", \"penguins\"]}\\n\\ndef generate_joke(state: OverallState):\\n    joke_map = {\\n        \"lions\": \"Why don\\'t lions like fast food? Because they can\\'t catch it!\",\\n        \"elephants\": \"Why don\\'t elephants use computers? They\\'re afraid of the mouse!\",\\n        \"penguins\": \"Why don\\'t penguins like talking to strangers at parties? Because they find it hard to break the ice.\"\\n    }\\n    return {\"jokes\": [joke_map[state[\"subject\"]]]}\\n\\ndef continue_to_jokes(state: OverallState):\\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]\\n\\ndef best_joke(state: OverallState):\\n    return {\"best_selected_joke\": \"penguins\"}\\n\\nbuilder = StateGraph(OverallState)\\nbuilder.add_node(\"generate_topics\", generate_topics)\\nbuilder.add_node(\"generate_joke\", generate_joke)\\nbuilder.add_node(\"best_joke\", best_joke)\\nbuilder.add_edge(START, \"generate_topics\")\\nbuilder.add_conditional_edges(\"generate_topics\", continue_to_jokes, [\"generate_joke\"])\\nbuilder.add_edge(\"generate_joke\", \"best_joke\")\\nbuilder.add_edge(\"best_joke\", END)\\ngraph = builder.compile()\\n\\nCopyAsk AIfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nCopyAsk AI# Call the graph: here we call it to generate a list of jokes\\nfor step in graph.stream({\"topic\": \"animals\"}):\\n    print(step)\\n\\nCopyAsk AI{\\'generate_topics\\': {\\'subjects\\': [\\'lions\\', \\'elephants\\', \\'penguins\\']}}\\n{\\'generate_joke\\': {\\'jokes\\': [\"Why don\\'t lions like fast food? Because they can\\'t catch it!\"]}}\\n{\\'generate_joke\\': {\\'jokes\\': [\"Why don\\'t elephants use computers? They\\'re afraid of the mouse!\"]}}\\n{\\'generate_joke\\': {\\'jokes\\': [\\'Why don\\'t penguins like talking to strangers at parties? Because they find it hard to break the ice.\\']}}\\n{\\'best_joke\\': {\\'best_selected_joke\\': \\'penguins\\'}}\\n\\n\\u200bCreate and control loops\\nWhen creating a graph with a loop, we require a mechanism for terminating execution. This is most commonly done by adding a conditional edge that routes to the END node once we reach some termination condition.\\nYou can also set the graph recursion limit when invoking or streaming the graph. The recursion limit sets the number of supersteps that the graph is allowed to execute before it raises an error. Read more about the concept of recursion limits here.\\nLet’s consider a simple graph with a loop to better understand how these mechanisms work.\\nTo return the last value of your state instead of receiving a recursion limit error, see the next section.\\nWhen creating a loop, you can include a conditional edge that specifies a termination condition:\\nCopyAsk AIbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\n\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if termination_condition(state):\\n        return END\\n    else:\\n        return \"b\"\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"a\")\\ngraph = builder.compile()\\n\\nTo control the recursion limit, specify \"recursionLimit\" in the config. This will raise a GraphRecursionError, which you can catch and handle:\\nCopyAsk AIfrom langgraph.errors import GraphRecursionError\\n\\ntry:\\n    graph.invoke(inputs, {\"recursion_limit\": 3})\\nexcept GraphRecursionError:\\n    print(\"Recursion Error\")\\n\\nLet’s define a graph with a simple loop. Note that we use a conditional edge to implement a termination condition.\\nCopyAsk AIimport operator\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Node A sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Node B sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\n# Define nodes\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\n\\n# Define edges\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if len(state[\"aggregate\"]) < 7:\\n        return \"b\"\\n    else:\\n        return END\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"a\")\\ngraph = builder.compile()\\n\\nCopyAsk AIfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nThis architecture is similar to a ReAct agent in which node \"a\" is a tool-calling model, and node \"b\" represents the tools.\\nIn our route conditional edge, we specify that we should end after the \"aggregate\" list in the state passes a threshold length.\\nInvoking the graph, we see that we alternate between nodes \"a\" and \"b\" before terminating once we reach the termination condition.\\nCopyAsk AIgraph.invoke({\"aggregate\": []})\\n\\nCopyAsk AINode A sees []\\nNode B sees [\\'A\\']\\nNode A sees [\\'A\\', \\'B\\']\\nNode B sees [\\'A\\', \\'B\\', \\'A\\']\\nNode A sees [\\'A\\', \\'B\\', \\'A\\', \\'B\\']\\nNode B sees [\\'A\\', \\'B\\', \\'A\\', \\'B\\', \\'A\\']\\nNode A sees [\\'A\\', \\'B\\', \\'A\\', \\'B\\', \\'A\\', \\'B\\']\\n\\n\\u200bImpose a recursion limit\\nIn some applications, we may not have a guarantee that we will reach a given termination condition. In these cases, we can set the graph’s recursion limit. This will raise a GraphRecursionError after a given number of supersteps. We can then catch and handle this exception:\\nCopyAsk AIfrom langgraph.errors import GraphRecursionError\\n\\ntry:\\n    graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\\nexcept GraphRecursionError:\\n    print(\"Recursion Error\")\\n\\nCopyAsk AINode A sees []\\nNode B sees [\\'A\\']\\nNode C sees [\\'A\\', \\'B\\']\\nNode D sees [\\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\']\\nRecursion Error\\n\\nExtended example: return state on hitting recursion limitInstead of raising GraphRecursionError, we can introduce a new key to the state that keeps track of the number of steps remaining until reaching the recursion limit. We can then use this key to determine if we should end the run.LangGraph implements a special RemainingSteps annotation. Under the hood, it creates a ManagedValue channel — a state channel that will exist for the duration of our graph run and no longer.CopyAsk AIimport operator\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.managed.is_last_step import RemainingSteps\\n\\nclass State(TypedDict):\\n    aggregate: Annotated[list, operator.add]\\n    remaining_steps: RemainingSteps\\n\\ndef a(state: State):\\n    print(f\\'Node A sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Node B sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\n# Define nodes\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\n\\n# Define edges\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if state[\"remaining_steps\"] <= 2:\\n        return END\\n    else:\\n        return \"b\"\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"a\")\\ngraph = builder.compile()\\n\\n# Test it out\\nresult = graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\\nprint(result)\\nCopyAsk AINode A sees []\\nNode B sees [\\'A\\']\\nNode A sees [\\'A\\', \\'B\\']\\n{\\'aggregate\\': [\\'A\\', \\'B\\', \\'A\\']}\\n\\nExtended example: loops with branchesTo better understand how the recursion limit works, let’s consider a more complex example. Below we implement a loop, but one step fans out into two nodes:CopyAsk AIimport operator\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Node A sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Node B sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef c(state: State):\\n    print(f\\'Node C sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\ndef d(state: State):\\n    print(f\\'Node D sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"D\"]}\\n\\n# Define nodes\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(c)\\nbuilder.add_node(d)\\n\\n# Define edges\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if len(state[\"aggregate\"]) < 7:\\n        return \"b\"\\n    else:\\n        return END\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"c\")\\nbuilder.add_edge(\"b\", \"d\")\\nbuilder.add_edge([\"c\", \"d\"], \"a\")\\ngraph = builder.compile()\\nCopyAsk AIfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\nThis graph looks complex, but can be conceptualized as loop of supersteps:\\nNode A\\nNode B\\nNodes C and D\\nNode A\\n…\\nWe have a loop of four supersteps, where nodes C and D are executed concurrently.Invoking the graph as before, we see that we complete two full “laps” before hitting the termination condition:CopyAsk AIresult = graph.invoke({\"aggregate\": []})\\nCopyAsk AINode A sees []\\nNode B sees [\\'A\\']\\nNode D sees [\\'A\\', \\'B\\']\\nNode C sees [\\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\']\\nNode B sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\']\\nNode D sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\', \\'B\\']\\nNode C sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\', \\'B\\', \\'C\\', \\'D\\']\\nHowever, if we set the recursion limit to four, we only complete one lap because each lap is four supersteps:CopyAsk AIfrom langgraph.errors import GraphRecursionError\\n\\ntry:\\n    result = graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\\nexcept GraphRecursionError:\\n    print(\"Recursion Error\")\\nCopyAsk AINode A sees []\\nNode B sees [\\'A\\']\\nNode C sees [\\'A\\', \\'B\\']\\nNode D sees [\\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\']\\nRecursion Error\\n\\n\\u200bAsync\\nUsing the async programming paradigm can produce significant performance improvements when running IO-bound code concurrently (e.g., making concurrent API requests to a chat model provider).\\nTo convert a sync implementation of the graph to an async implementation, you will need to:\\n\\nUpdate nodes use async def instead of def.\\nUpdate the code inside to use await appropriately.\\nInvoke the graph with .ainvoke or .astream as desired.\\n\\nBecause many LangChain objects implement the Runnable Protocol which has async variants of all the sync methods it’s typically fairly quick to upgrade a sync graph to an async graph.\\nSee example below. To demonstrate async invocations of underlying LLMs, we will include a chat model:\\n OpenAI Anthropic Azure Google Gemini AWS Bedrock👉 Read the OpenAI chat model integration docsCopyAsk AIpip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyAsk AIimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\n\\nCopyAsk AIfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import MessagesState, StateGraph\\n\\nasync def node(state: MessagesState):  \\n    new_message = await llm.ainvoke(state[\"messages\"])  \\n    return {\"messages\": [new_message]}\\n\\nbuilder = StateGraph(MessagesState).add_node(node).set_entry_point(\"node\")\\ngraph = builder.compile()\\n\\ninput_message = {\"role\": \"user\", \"content\": \"Hello\"}\\nresult = await graph.ainvoke({\"messages\": [input_message]})  \\n\\nAsync streaming\\nSee the streaming guide for examples of streaming with async.\\n\\u200bCombine control flow and state updates with Command\\nIt can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a Command object from node functions:\\nCopyAsk AIdef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    return Command(\\n        # state update\\n        update={\"foo\": \"bar\"},\\n        # control flow\\n        goto=\"my_other_node\"\\n    )\\n\\nWe show an end-to-end example below. Let’s create a simple graph with 3 nodes: A, B and C. We will first execute node A, and then decide whether to go to Node B or Node C next based on the output of node A.\\nCopyAsk AIimport random\\nfrom typing_extensions import TypedDict, Literal\\nfrom langgraph.graph import StateGraph, START\\nfrom langgraph.types import Command\\n\\n# Define graph state\\nclass State(TypedDict):\\n    foo: str\\n\\n# Define the nodes\\n\\ndef node_a(state: State) -> Command[Literal[\"node_b\", \"node_c\"]]:\\n    print(\"Called A\")\\n    value = random.choice([\"b\", \"c\"])\\n    # this is a replacement for a conditional edge function\\n    if value == \"b\":\\n        goto = \"node_b\"\\n    else:\\n        goto = \"node_c\"\\n\\n    # note how Command allows you to BOTH update the graph state AND route to the next node\\n    return Command(\\n        # this is the state update\\n        update={\"foo\": value},\\n        # this is a replacement for an edge\\n        goto=goto,\\n    )\\n\\ndef node_b(state: State):\\n    print(\"Called B\")\\n    return {\"foo\": state[\"foo\"] + \"b\"}\\n\\ndef node_c(state: State):\\n    print(\"Called C\")\\n    return {\"foo\": state[\"foo\"] + \"c\"}\\n\\nWe can now create the StateGraph with the above nodes. Notice that the graph doesn’t have conditional edges for routing! This is because control flow is defined with Command inside node_a.\\nCopyAsk AIbuilder = StateGraph(State)\\nbuilder.add_edge(START, \"node_a\")\\nbuilder.add_node(node_a)\\nbuilder.add_node(node_b)\\nbuilder.add_node(node_c)\\n# NOTE: there are no edges between nodes A, B and C!\\n\\ngraph = builder.compile()\\n\\nYou might have noticed that we used Command as a return type annotation, e.g. Command[Literal[\"node_b\", \"node_c\"]]. This is necessary for the graph rendering and tells LangGraph that node_a can navigate to node_b and node_c.\\nCopyAsk AIfrom IPython.display import display, Image\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nIf we run the graph multiple times, we’d see it take different paths (A -> B or A -> C) based on the random choice in node A.\\nCopyAsk AIgraph.invoke({\"foo\": \"\"})\\n\\nCopyAsk AICalled A\\nCalled C\\n\\n\\u200bNavigate to a node in a parent graph\\nIf you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify graph=Command.PARENT in Command:\\nCopyAsk AIdef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    return Command(\\n        update={\"foo\": \"bar\"},\\n        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\\n        graph=Command.PARENT\\n    )\\n\\nLet’s demonstrate this using the above example. We’ll do so by changing nodeA in the above example into a single-node graph that we’ll add as a subgraph to our parent graph.\\nState updates with Command.PARENT\\nWhen you send updates from a subgraph node to a parent graph node for a key that’s shared by both parent and subgraph state schemas, you must define a reducer for the key you’re updating in the parent graph state. See the example below.\\nCopyAsk AIimport operator\\nfrom typing_extensions import Annotated\\n\\nclass State(TypedDict):\\n    # NOTE: we define a reducer here\\n    foo: Annotated[str, operator.add]  \\n\\ndef node_a(state: State):\\n    print(\"Called A\")\\n    value = random.choice([\"a\", \"b\"])\\n    # this is a replacement for a conditional edge function\\n    if value == \"a\":\\n        goto = \"node_b\"\\n    else:\\n        goto = \"node_c\"\\n\\n    # note how Command allows you to BOTH update the graph state AND route to the next node\\n    return Command(\\n        update={\"foo\": value},\\n        goto=goto,\\n        # this tells LangGraph to navigate to node_b or node_c in the parent graph\\n        # NOTE: this will navigate to the closest parent graph relative to the subgraph\\n        graph=Command.PARENT,  \\n    )\\n\\nsubgraph = StateGraph(State).add_node(node_a).add_edge(START, \"node_a\").compile()\\n\\ndef node_b(state: State):\\n    print(\"Called B\")\\n    # NOTE: since we\\'ve defined a reducer, we don\\'t need to manually append\\n    # new characters to existing \\'foo\\' value. instead, reducer will append these\\n    # automatically (via operator.add)\\n    return {\"foo\": \"b\"}  \\n\\ndef node_c(state: State):\\n    print(\"Called C\")\\n    return {\"foo\": \"c\"}  \\n\\nbuilder = StateGraph(State)\\nbuilder.add_edge(START, \"subgraph\")\\nbuilder.add_node(\"subgraph\", subgraph)\\nbuilder.add_node(node_b)\\nbuilder.add_node(node_c)\\n\\ngraph = builder.compile()\\n\\nCopyAsk AIgraph.invoke({\"foo\": \"\"})\\n\\nCopyAsk AICalled A\\nCalled C\\n\\n\\u200bUse inside tools\\nA common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation. To update the graph state from the tool, you can return Command(update={\"my_custom_key\": \"foo\", \"messages\": [...]}) from the tool:\\nCopyAsk AI@tool\\ndef lookup_user_info(tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig):\\n    \"\"\"Use this to look up user information to better assist them with their questions.\"\"\"\\n    user_info = get_user_info(config.get(\"configurable\", {}).get(\"user_id\"))\\n    return Command(\\n        update={\\n            # update the state keys\\n            \"user_info\": user_info,\\n            # update the message history\\n            \"messages\": [ToolMessage(\"Successfully looked up user information\", tool_call_id=tool_call_id)]\\n        }\\n    )\\n\\nYou MUST include messages (or any state key used for the message history) in Command.update when returning Command from a tool and the list of messages in messages MUST contain a ToolMessage. This is necessary for the resulting message history to be valid (LLM providers require AI messages with tool calls to be followed by the tool result messages).\\nIf you are using tools that update state via Command, we recommend using prebuilt ToolNode which automatically handles tools returning Command objects and propagates them to the graph state. If you’re writing a custom node that calls tools, you would need to manually propagate Command objects returned by the tools as the update from the node.\\n\\u200bVisualize your graph\\nHere we demonstrate how to visualize the graphs you create.\\nYou can visualize any arbitrary Graph, including StateGraph.\\nLet’s have some fun by drawing fractals :).\\nCopyAsk AIimport random\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.graph.message import add_messages\\n\\nclass State(TypedDict):\\n    messages: Annotated[list, add_messages]\\n\\nclass MyNode:\\n    def __init__(self, name: str):\\n        self.name = name\\n    def __call__(self, state: State):\\n        return {\"messages\": [(\"assistant\", f\"Called node {self.name}\")]}\\n\\ndef route(state) -> Literal[\"entry_node\", END]:\\n    if len(state[\"messages\"]) > 10:\\n        return END\\n    return \"entry_node\"\\n\\ndef add_fractal_nodes(builder, current_node, level, max_level):\\n    if level > max_level:\\n        return\\n    # Number of nodes to create at this level\\n    num_nodes = random.randint(1, 3)  # Adjust randomness as needed\\n    for i in range(num_nodes):\\n        nm = [\"A\", \"B\", \"C\"][i]\\n        node_name = f\"node_{current_node}_{nm}\"\\n        builder.add_node(node_name, MyNode(node_name))\\n        builder.add_edge(current_node, node_name)\\n        # Recursively add more nodes\\n        r = random.random()\\n        if r > 0.2 and level + 1 < max_level:\\n            add_fractal_nodes(builder, node_name, level + 1, max_level)\\n        elif r > 0.05:\\n            builder.add_conditional_edges(node_name, route, node_name)\\n        else:\\n            # End\\n            builder.add_edge(node_name, END)\\n\\ndef build_fractal_graph(max_level: int):\\n    builder = StateGraph(State)\\n    entry_point = \"entry_node\"\\n    builder.add_node(entry_point, MyNode(entry_point))\\n    builder.add_edge(START, entry_point)\\n    add_fractal_nodes(builder, entry_point, 1, max_level)\\n    # Optional: set a finish point if required\\n    builder.add_edge(entry_point, END)  # or any specific node\\n    return builder.compile()\\n\\napp = build_fractal_graph(3)\\n\\n\\u200bMermaid\\nWe can also convert a graph class into Mermaid syntax.\\nCopyAsk AIprint(app.get_graph().draw_mermaid())\\n\\nCopyAsk AI%%{init: {\\'flowchart\\': {\\'curve\\': \\'linear\\'}}}%%\\ngraph TD;\\n    tart__([<p>__start__</p>]):::first\\n    ry_node(entry_node)\\n    e_entry_node_A(node_entry_node_A)\\n    e_entry_node_B(node_entry_node_B)\\n    e_node_entry_node_B_A(node_node_entry_node_B_A)\\n    e_node_entry_node_B_B(node_node_entry_node_B_B)\\n    e_node_entry_node_B_C(node_node_entry_node_B_C)\\n    nd__([<p>__end__</p>]):::last\\n    tart__ --> entry_node;\\n    ry_node --> __end__;\\n    ry_node --> node_entry_node_A;\\n    ry_node --> node_entry_node_B;\\n    e_entry_node_B --> node_node_entry_node_B_A;\\n    e_entry_node_B --> node_node_entry_node_B_B;\\n    e_entry_node_B --> node_node_entry_node_B_C;\\n    e_entry_node_A -.-> entry_node;\\n    e_entry_node_A -.-> __end__;\\n    e_node_entry_node_B_A -.-> entry_node;\\n    e_node_entry_node_B_A -.-> __end__;\\n    e_node_entry_node_B_B -.-> entry_node;\\n    e_node_entry_node_B_B -.-> __end__;\\n    e_node_entry_node_B_C -.-> entry_node;\\n    e_node_entry_node_B_C -.-> __end__;\\n    ssDef default fill:#f2f0ff,line-height:1.2\\n    ssDef first fill-opacity:0\\n    ssDef last fill:#bfb6fc\\n\\n\\u200bPNG\\nIf preferred, we could render the Graph into a .png. Here we could use three options:\\n\\nUsing Mermaid.ink API (does not require additional packages)\\nUsing Mermaid + Pyppeteer (requires pip install pyppeteer)\\nUsing graphviz (which requires pip install graphviz)\\n\\nUsing Mermaid.Ink\\nBy default, draw_mermaid_png() uses Mermaid.Ink’s API to generate the diagram.\\nCopyAsk AIfrom IPython.display import Image, display\\nfrom langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\\n\\ndisplay(Image(app.get_graph().draw_mermaid_png()))\\n\\n\\nUsing Mermaid + Pyppeteer\\nCopyAsk AIimport nest_asyncio\\n\\nnest_asyncio.apply()  # Required for Jupyter Notebook to run async functions\\n\\ndisplay(\\n    Image(\\n        app.get_graph().draw_mermaid_png(\\n            curve_style=CurveStyle.LINEAR,\\n            node_colors=NodeStyles(first=\"#ffdfba\", last=\"#baffc9\", default=\"#fad7de\"),\\n            wrap_label_n_words=9,\\n            output_file_path=None,\\n            draw_method=MermaidDrawMethod.PYPPETEER,\\n            background_color=\"white\",\\n            padding=10,\\n        )\\n    )\\n)\\n\\nUsing Graphviz\\nCopyAsk AItry:\\n    display(Image(app.get_graph().draw_png()))\\nexcept ImportError:\\n    print(\\n        \"You likely need to install dependencies for pygraphviz, see more here https://github.com/pygraphviz/pygraphviz/blob/main/INSTALL.txt\"\\n    )\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoGraph API overviewPreviousFunctional API overviewNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='LangGraph overview - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationLangGraph overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this page InstallCore benefitsLangGraph ecosystemAcknowledgementsLangGraph overviewCopy pageCopy pageLangGraph v1.0 is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.\\nTrusted by companies shaping the future of agents— including Klarna, Replit, Elastic, and more— LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\\nLangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.\\nWe will commonly use LangChain components throughout the documentation to integrate models and tools, but you don’t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain’s agents that provide pre-built architectures for common LLM and tool-calling loops.\\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\n\\u200b Install\\npipuvCopyAsk AIpip install -U langgraph\\n\\nThen, create a simple hello world example:\\nCopyAsk AIfrom langgraph.graph import StateGraph, MessagesState, START, END\\n\\ndef mock_llm(state: MessagesState):\\n    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\\n\\ngraph = StateGraph(MessagesState)\\ngraph.add_node(mock_llm)\\ngraph.add_edge(START, \"mock_llm\")\\ngraph.add_edge(\"mock_llm\", END)\\ngraph = graph.compile()\\n\\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\\n\\n\\u200bCore benefits\\nLangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\\n\\nDurable execution: Build agents that persist through failures and can run for extended periods, resuming from where they left off.\\nHuman-in-the-loop: Incorporate human oversight by inspecting and modifying agent state at any point.\\nComprehensive memory: Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\\nDebugging with LangSmith: Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\\nProduction-ready deployment: Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\\n\\n\\u200bLangGraph ecosystem\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\\n\\nLangSmith — Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.\\nLangSmith — Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams — and iterate quickly with visual prototyping in Studio.\\nLangChain - Provides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.\\n\\n\\u200bAcknowledgements\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoWhat\\'s new in v1Next⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/local-server#python-sdk-async', 'title': 'Run a local server - Docs by LangChain', 'language': 'en'}, page_content='Run a local server - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationGet startedRun a local serverLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pagePrerequisites1. Install the LangGraph CLI2. Create a LangGraph app 🌱3. Install dependencies4. Create a .env file5. Launch Agent server 🚀6. Test your application in Studio7. Test the APINext stepsGet startedRun a local serverCopy pageCopy pageThis guide shows you how to run a LangGraph application locally.\\n\\u200bPrerequisites\\nBefore you begin, ensure you have the following:\\n\\nAn API key for LangSmith - free to sign up\\n\\n\\u200b1. Install the LangGraph CLI\\npipuvCopyAsk AI# Python >= 3.11 is required.\\npip install -U \"langgraph-cli[inmem]\"\\n\\n\\u200b2. Create a LangGraph app 🌱\\nCreate a new app from the new-langgraph-project-python template. This template demonstrates a single-node application you can extend with your own logic.\\nCopyAsk AIlanggraph new path/to/your/app --template new-langgraph-project-python\\n\\nAdditional templates\\nIf you use langgraph new without specifying a template, you will be presented with an interactive menu that will allow you to choose from a list of available templates.\\n\\u200b3. Install dependencies\\nIn the root of your new LangGraph app, install the dependencies in edit mode so your local changes are used by the server:\\npipuvCopyAsk AIcd path/to/your/app\\npip install -e .\\n\\n\\u200b4. Create a .env file\\nYou will find a .env.example in the root of your new LangGraph app. Create a .env file in the root of your new LangGraph app and copy the contents of the .env.example file into it, filling in the necessary API keys:\\nCopyAsk AILANGSMITH_API_KEY=lsv2...\\n\\n\\u200b5. Launch Agent server 🚀\\nStart the LangGraph API server locally:\\nCopyAsk AIlanggraph dev\\n\\nSample output:\\nCopyAsk AI>    Ready!\\n>\\n>    - API: [http://localhost:2024](http://localhost:2024/)\\n>\\n>    - Docs: http://localhost:2024/docs\\n>\\n>    - LangGraph Studio Web UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\\n\\nThe langgraph dev command starts Agent Server in an in-memory mode. This mode is suitable for development and testing purposes. For production use, deploy Agent Server with access to a persistent storage backend. For more information, see the Platform setup overview.\\n\\u200b6. Test your application in Studio\\nStudio is a specialized UI that you can connect to LangGraph API server to visualize, interact with, and debug your application locally. Test your graph in Studio by visiting the URL provided in the output of the langgraph dev command:\\nCopyAsk AI>    - LangGraph Studio Web UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\\n\\nFor an Agent Server running on a custom host/port, update the baseURL parameter.\\nSafari compatibilityUse the --tunnel flag with your command to create a secure tunnel, as Safari has limitations when connecting to localhost servers:CopyAsk AIlanggraph dev --tunnel\\n\\n\\u200b7. Test the API\\n Python SDK (async) Python SDK (sync) Rest API\\nInstall the LangGraph Python SDK:\\nCopyAsk AIpip install langgraph-sdk\\n\\nSend a message to the assistant (threadless run):\\nCopyAsk AIfrom langgraph_sdk import get_client\\nimport asyncio\\n\\nclient = get_client(url=\"http://localhost:2024\")\\n\\nasync def main():\\n    async for chunk in client.runs.stream(\\n        None,  # Threadless run\\n        \"agent\", # Name of assistant. Defined in langgraph.json.\\n        input={\\n        \"messages\": [{\\n            \"role\": \"human\",\\n            \"content\": \"What is LangGraph?\",\\n            }],\\n        },\\n    ):\\n        print(f\"Receiving new event of type: {chunk.event}...\")\\n        print(chunk.data)\\n        print(\"\\\\n\\\\n\")\\n\\nasyncio.run(main())\\n\\n\\u200bNext steps\\nNow that you have a LangGraph app running locally, take your journey further by exploring deployment and advanced features:\\n\\n\\nDeployment quickstart: Deploy your LangGraph app using LangSmith.\\n\\n\\nLangSmith: Learn about foundational LangSmith concepts.\\n\\n\\nPython SDK Reference: Explore the Python SDK API Reference.\\n\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoQuickstartPreviousThinking in LangGraphNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/use-graph-api#map-reduce-and-the-send-api\",\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/overview\",\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/local-server#python-sdk-async\"\n",
    "]\n",
    "\n",
    "loaders = [WebBaseLoader(url) for url in urls]\n",
    "docs = []\n",
    "\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3fb340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "vectorstorelanggraph = FAISS.from_documents(chunks, embeddings)\n",
    "retrieverlanggraph = vectorstorelanggraph.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca289b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool_langgraph = create_retriever_tool(\n",
    "    retrieverlanggraph,\n",
    "    \"retriever_langgraph_docs\",\n",
    "    \"Search and run information about LangGraph\"\n",
    ")\n",
    "\n",
    "tools = [retriever_tool, retriever_tool_langgraph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c1af8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "from typing import Literal\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[AnyMessage], add_messages]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b2322c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state):\n",
    "    \"\"\"AgentStateInvokes the agent model to generate a response based on the current state. Given the question, it will decide whether to retrieve using the retriever tool or simply end.\n",
    "    \n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "        \n",
    "    Returns:\n",
    "        dict: The updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state['messages']\n",
    "    llm = llm.bind_tools(tools)\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a87e458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field\n",
    "from langchain_classic.prompts import PromptTemplate\n",
    "\n",
    "#Edges\n",
    "def grade_documents(state)-> Literal[\"generate\",\"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relavant to tjhe question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): current state\n",
    "\n",
    "    Returns: \n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance checks\"\"\"\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    model = init_chat_model(\"groq:openai/gpt-oss-20b\")\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    prompt = PromptTemplate(template=\"\"\"You are a grader assessing the relevance of a retrieved document to a user question. \\n\n",
    "                            Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "                            Here is the user question: {question} \\n\n",
    "                            If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "                            Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "                            \"\"\",\n",
    "                            input_variables=[\"context\",\"question\"]\n",
    "                            )\n",
    "    \n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DESISION: DOCS RELEVANT---\")\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        print(\"---DESISION: DOCS NOT RELEVANT---\")\n",
    "        return \"rewrite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cebfa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state(messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated message\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    model = init_chat_model(\"groq:openai/gpt-oss-20b\")\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    rag_chain = prompt | model | StrOutputParser()\n",
    "\n",
    "    res = rag_chain.invoke({\"context\": format_docs(docs), \"question\": question})\n",
    "    return {\"messages\":[res]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78a4f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "         state(messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: the updated state with re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state['messages']\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n\n",
    "            Look at the input and try to reason about the underlying semantic intent / meaning. \\n\n",
    "\n",
    "            Here is the initial question:\n",
    "            \\n ------ \\n\n",
    "            {question}\n",
    "            \\n ------ \\n\n",
    "            Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    model = init_chat_model(\"groq:openai/gpt-oss-20b\")\n",
    "    res = model.invoke(msg)\n",
    "    \n",
    "    return {'messages': [res]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cfff2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The first argument must be a string or a callable with a __name__ for tool decorator. Got <class 'langchain_core.vectorstores.base.VectorStoreRetriever'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m builder \u001b[38;5;241m=\u001b[39m StateGraph(AgentState)\n\u001b[0;32m      6\u001b[0m builder\u001b[38;5;241m.\u001b[39madd_node(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent\u001b[39m\u001b[38;5;124m\"\u001b[39m, agent)\n\u001b[1;32m----> 7\u001b[0m retrieve \u001b[38;5;241m=\u001b[39m \u001b[43mToolNode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mretriever_tool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrieverlanggraph\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m builder\u001b[38;5;241m.\u001b[39madd_node(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrieve\u001b[39m\u001b[38;5;124m\"\u001b[39m, retrieve)\n\u001b[0;32m      9\u001b[0m builder\u001b[38;5;241m.\u001b[39madd_node(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrewrite\u001b[39m\u001b[38;5;124m\"\u001b[39m, rewrite)\n",
      "File \u001b[1;32md:\\RAG\\AgenticAI-LangGraph\\.venv\\Lib\\site-packages\\langgraph\\prebuilt\\tool_node.py:677\u001b[0m, in \u001b[0;36mToolNode.__init__\u001b[1;34m(self, tools, name, tags, handle_tool_errors, messages_key, wrap_tool_call, awrap_tool_call)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m tools:\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool, BaseTool):\n\u001b[1;32m--> 677\u001b[0m         tool_ \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_tool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype[BaseTool]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m         tool_ \u001b[38;5;241m=\u001b[39m tool\n",
      "File \u001b[1;32md:\\RAG\\AgenticAI-LangGraph\\.venv\\Lib\\site-packages\\langchain_core\\tools\\convert.py:356\u001b[0m, in \u001b[0;36mtool\u001b[1;34m(name_or_callable, runnable, description, return_direct, args_schema, infer_schema, response_format, parse_docstring, error_on_invalid_docstring, *args)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _create_tool_factory(name_or_callable)\n\u001b[0;32m    352\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    353\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe first argument must be a string or a callable with a __name__ \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    354\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor tool decorator. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(name_or_callable)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    355\u001b[0m     )\n\u001b[1;32m--> 356\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    358\u001b[0m \u001b[38;5;66;03m# Tool is used as a decorator with parameters specified\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;66;03m# @tool(parse_docstring=True)\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;66;03m# def my_tool():\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;66;03m#    pass\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_partial\u001b[39m(func: Callable \u001b[38;5;241m|\u001b[39m Runnable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseTool:\n",
      "\u001b[1;31mValueError\u001b[0m: The first argument must be a string or a callable with a __name__ for tool decorator. Got <class 'langchain_core.vectorstores.base.VectorStoreRetriever'>"
     ]
    }
   ],
   "source": [
    "# Build Langgraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "builder.add_node(\"agent\", agent)\n",
    "retrieve = ToolNode(tools=tools)\n",
    "builder.add_node(\"retrieve\", retrieve)\n",
    "builder.add_node(\"rewrite\", rewrite)\n",
    "builder.add_node(\"generate\", generate)\n",
    "\n",
    "builder.set_entry_point(\"agent\")\n",
    "builder.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tools_condition,\n",
    "    {\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    'retrieve',\n",
    "    grade_documents\n",
    ")\n",
    "\n",
    "builder.add_edge(\"generate\",END)\n",
    "builder.add_edge(\"rewrite\",\"agent\")\n",
    "\n",
    "graph = builder.compile()\n",
    "graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AgenticAI-LangGraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
