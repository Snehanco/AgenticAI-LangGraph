{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a86aa13",
   "metadata": {},
   "source": [
    "### What is Cache-Augmented Generation (CAG)?\n",
    "CAG is a retrieval-free approach that bypasses the usual step of querying external knowledge sources at inference time. Instead, it preloads relevant documents into the LLM's extended context window, precomputes the model‚Äôs key‚Äëvalue (KV) cache, and reuses this during inference‚Äîso the model can generate responses without additional retrieval steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03e6e537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RAG\\AgenticAI-LangGraph\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000020D0A3B42D0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000020D0A4F1B90>, model_name='openai/gpt-oss-20b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm=init_chat_model(\"groq:openai/gpt-oss-20b\")\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12d950ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cache variable\n",
    "Model_Cache={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48802156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def cache_model(query):\n",
    "    start_time=time.time()\n",
    "    if Model_Cache.get(query):\n",
    "        print(\"**CAche Hit**\")\n",
    "        end_time=time.time()\n",
    "        elapsed_time=end_time-start_time\n",
    "        print(f\"EXECUTION TIME: {elapsed_time:.2f} seconds\")\n",
    "        return Model_Cache.get(query)\n",
    "    else:\n",
    "        print(\"***CACHE MISS ‚Äì EXECUTING MODEL***\")\n",
    "        start_time = time.time()\n",
    "        response = llm.invoke(query)\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        print(f\"EXECUTION TIME: {elapsed:.2f} seconds\")\n",
    "        Model_Cache[query] = response\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85b3c6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**CAche Hit**\n",
      "EXECUTION TIME: 0.00 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! üëã How can I help you today?', additional_kwargs={'reasoning_content': 'User says \"hi\". We need to respond with a friendly greeting. Possibly ask how can help.'}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 72, 'total_tokens': 113, 'completion_time': 0.041369398, 'prompt_time': 0.003449039, 'queue_time': 0.1558129, 'total_time': 0.044818437, 'completion_tokens_details': {'reasoning_tokens': 21}}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_a12402de73', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--3619fad1-0f78-4c58-bcac-85303b942ad2-0', usage_metadata={'input_tokens': 72, 'output_tokens': 41, 'total_tokens': 113})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=cache_model(\"hi\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6407acf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hi': AIMessage(content='Hello! üëã How can I help you today?', additional_kwargs={'reasoning_content': 'User says \"hi\". We need to respond with a friendly greeting. Possibly ask how can help.'}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 72, 'total_tokens': 113, 'completion_time': 0.041369398, 'prompt_time': 0.003449039, 'queue_time': 0.1558129, 'total_time': 0.044818437, 'completion_tokens_details': {'reasoning_tokens': 21}}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_a12402de73', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--3619fad1-0f78-4c58-bcac-85303b942ad2-0', usage_metadata={'input_tokens': 72, 'output_tokens': 41, 'total_tokens': 113})}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model_Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32188896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**CAche Hit**\n",
      "EXECUTION TIME: 0.00 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! üëã How can I help you today?', additional_kwargs={'reasoning_content': 'User says \"hi\". We need to respond with a friendly greeting. Possibly ask how can help.'}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 72, 'total_tokens': 113, 'completion_time': 0.041369398, 'prompt_time': 0.003449039, 'queue_time': 0.1558129, 'total_time': 0.044818437, 'completion_tokens_details': {'reasoning_tokens': 21}}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_a12402de73', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--3619fad1-0f78-4c58-bcac-85303b942ad2-0', usage_metadata={'input_tokens': 72, 'output_tokens': 41, 'total_tokens': 113})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=cache_model(\"hi\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d65ff0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***CACHE MISS ‚Äì EXECUTING MODEL***\n",
      "EXECUTION TIME: 1.95 seconds\n",
      "content='**LangGraph: Building Language‚ÄëModel Applications with Graph‚ÄëBased Architecture**\\n\\nLangGraph is a cutting‚Äëedge framework released by the LangChain team that reimagines how we compose, orchestrate, and deploy language‚Äëmodel (LLM) applications. Instead of the traditional linear ‚Äúprompt‚Äëthen‚Äëresponse‚Äù pipeline, LangGraph treats every step of an LLM workflow as a node in a directed graph. This paradigm shift brings modularity, observability, and scalability to the forefront, enabling developers to build complex, multi‚Äëstep reasoning engines that can be easily debugged, extended, and deployed.\\n\\n---\\n\\n### Core Concepts\\n\\n| Component | Description |\\n|-----------|-------------|\\n| **Node** | A self‚Äëcontained unit of computation. A node can be a prompt, a function call, a data fetch, or any LLM inference step. |\\n| **Edge** | The flow of data between nodes. Edges can carry text, structured JSON, or even arbitrary Python objects. |\\n| **Graph** | The entire network of nodes and edges. Graphs can be acyclic or contain loops, allowing for iterative refinement. |\\n| **State** | A shared, mutable context that nodes read from and write to. Think of it as a conversation memory that persists across steps. |\\n| **Executor** | The runtime that traverses the graph, resolving dependencies, handling async I/O, and managing retries. |\\n\\n---\\n\\n### How It Works\\n\\n1. **Define Nodes** ‚Äì Each node encapsulates a specific task. For example, a `Summarizer` node might take a chunk of text and return a concise summary; a `Retriever` node might query a vector store. Nodes are implemented as simple Python callables or subclasses of `BaseNode`, and can be decorated with `@langgraph.node`.\\n\\n2. **Connect Nodes** ‚Äì Edges are declared declaratively using the `@graph` decorator. You specify which node outputs feed into which inputs, and optionally provide conditions that determine the next node (e.g., if the summary is too short, loop back to the retriever).\\n\\n3. **Manage State** ‚Äì The graph‚Äôs state is a dictionary that persists across nodes. You can store intermediate results, flags, or even entire conversation histories. This shared state is crucial for multi‚Äëturn dialogue or chain‚Äëof‚Äëthought reasoning.\\n\\n4. **Execute** ‚Äì The `Executor` walks the graph from the designated start node, invoking each node in turn. It handles asynchronous execution, error handling, and can be paused or resumed. The final output is typically the value of a designated terminal node.\\n\\n---\\n\\n### Real‚ÄëWorld Use Cases\\n\\n- **Multi‚ÄëStep Question Answering** ‚Äì A graph can first retrieve relevant documents, then ask the LLM to extract key facts, and finally synthesize an answer. The state holds the retrieved snippets, making it easy to audit each step.\\n\\n- **Conversational Agents** ‚Äì By embedding a `Turn` node that captures user input and a `Response` node that generates replies, the graph can maintain context over long sessions. The state can store a conversation history that is fed back into the prompt.\\n\\n- **Data‚ÄëDriven Workflows** ‚Äì Combine LLM inference with external APIs. For instance, a node could call a weather API, another could ask the LLM to interpret the data, and a final node could generate a user‚Äëfriendly report.\\n\\n- **Iterative Refinement** ‚Äì Loops allow the system to refine outputs. A `Rewriter` node can improve the fluency of an LLM‚Äôs draft, and a `Validator` node can check for factual correctness, looping until a quality threshold is met.\\n\\n---\\n\\n### Advantages Over Traditional Prompt Pipelines\\n\\n1. **Modularity** ‚Äì Nodes can be reused across graphs. A `Summarizer` node built for one project can be dropped into another without modification.\\n\\n2. **Observability** ‚Äì Each node‚Äôs input and output can be logged, enabling fine‚Äëgrained debugging. GraphViz integration lets you visualize the entire workflow.\\n\\n3. **Scalability** ‚Äì Graphs can be distributed across multiple workers or even micro‚Äëservices. The executor can schedule nodes on GPU clusters or serverless functions.\\n\\n4. **Extensibility** ‚Äì Adding a new capability (e.g., a new retrieval backend) involves writing a single node and wiring it into the graph.\\n\\n5. **Declarative Flow Control** ‚Äì Conditional logic is expressed at the graph level, not inside prompts. This reduces prompt clutter and makes the control flow explicit.\\n\\n---\\n\\n### Getting Started\\n\\n```python\\nfrom langgraph import Graph, node, executor\\n\\n@node\\ndef retrieve(query: str) -> str:\\n    # Example: call a vector store\\n    return vector_store.search(query)\\n\\n@node\\ndef summarize(text: str) -> str:\\n    return llm.generate(f\"Summarize: {text}\")\\n\\n@node\\ndef answer(summary: str, question: str) -> str:\\n    return llm.generate(f\"Answer the question \\'{question}\\' based on: {summary}\")\\n\\ngraph = Graph()\\ngraph.add_node(retrieve, name=\"Retriever\")\\ngraph.add_node(summarize, name=\"Summarizer\")\\ngraph.add_node(answer, name=\"Answerer\")\\n\\ngraph.connect(\"Retriever\", \"Summarizer\", inputs={\"text\": \"output\"})\\ngraph.connect(\"Summarizer\", \"Answerer\", inputs={\"summary\": \"output\"})\\ngraph.set_start(\"Retriever\")\\n\\nexecutor = executor.Executor(graph)\\nresult = executor.run({\"query\": \"What caused the fall of the Roman Empire?\"})\\nprint(result[\"Answerer\"])\\n```\\n\\nRunning this snippet will trigger the retrieval, summarization, and answering steps automatically, with the state carrying data between them.\\n\\n---\\n\\n### Limitations & Future Directions\\n\\n- **Complex Graphs Can Be Hard to Visualize** ‚Äì While GraphViz helps, very large graphs may still be opaque. Tooling for hierarchical views is an active area of development.\\n\\n- **Learning Curve** ‚Äì New developers must grasp both LLM fundamentals and graph theory concepts. Comprehensive documentation and tutorials are expanding.\\n\\n- **State Management Overhead** ‚Äì Persisting large state objects can be memory‚Äëintensive. Future releases aim to integrate more efficient serialization and partial state updates.\\n\\n- **Integration with Other Frameworks** ‚Äì LangGraph is designed to interoperate with LangChain, but bridging to non‚ÄëPython ecosystems (e.g., JavaScript) will broaden its reach.\\n\\n---\\n\\n### Conclusion\\n\\nLangGraph represents a paradigm shift in how we build LLM‚Äëpowered applications. By treating every inference step as a node in a graph, it brings modularity, transparency, and scalability to the forefront. Whether you‚Äôre prototyping a chatbot, orchestrating a data‚Äëdriven decision engine, or building a multi‚Äëstep reasoning pipeline, LangGraph offers a flexible, declarative framework that makes complex LLM workflows manageable and maintainable. As the ecosystem matures, expect richer tooling, tighter integration with vector databases, and even more powerful abstractions that bring the full power of graph theory to the AI developer‚Äôs toolkit.' additional_kwargs={'reasoning_content': \"We need to produce a 500-word article on LangGraph. The user wants 500 words. We need to ensure we hit that length. Let's consider what LangGraph is: It's a framework for building language model applications with graph-based architectures, from LangChain. Provide details: architecture, components, use-cases, how it works, how to use, examples, advantages, limitations, future directions. Must be around 500 words. Let's target ~520 words to be safe. We'll write a coherent piece.\"} response_metadata={'token_usage': {'completion_tokens': 1549, 'prompt_tokens': 82, 'total_tokens': 1631, 'completion_time': 1.655516733, 'prompt_time': 0.032100409, 'queue_time': 0.16649755, 'total_time': 1.6876171420000001, 'completion_tokens_details': {'reasoning_tokens': 104}}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--fbfe8395-4cec-4231-b207-93ed2ad7a564-0' usage_metadata={'input_tokens': 82, 'output_tokens': 1549, 'total_tokens': 1631}\n"
     ]
    }
   ],
   "source": [
    "query=\"can you give me 500 words on langgraph?\"\n",
    "response =cache_model(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b37e3685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**CAche Hit**\n",
      "EXECUTION TIME: 0.00 seconds\n",
      "content='**LangGraph: Building Language‚ÄëModel Applications with Graph‚ÄëBased Architecture**\\n\\nLangGraph is a cutting‚Äëedge framework released by the LangChain team that reimagines how we compose, orchestrate, and deploy language‚Äëmodel (LLM) applications. Instead of the traditional linear ‚Äúprompt‚Äëthen‚Äëresponse‚Äù pipeline, LangGraph treats every step of an LLM workflow as a node in a directed graph. This paradigm shift brings modularity, observability, and scalability to the forefront, enabling developers to build complex, multi‚Äëstep reasoning engines that can be easily debugged, extended, and deployed.\\n\\n---\\n\\n### Core Concepts\\n\\n| Component | Description |\\n|-----------|-------------|\\n| **Node** | A self‚Äëcontained unit of computation. A node can be a prompt, a function call, a data fetch, or any LLM inference step. |\\n| **Edge** | The flow of data between nodes. Edges can carry text, structured JSON, or even arbitrary Python objects. |\\n| **Graph** | The entire network of nodes and edges. Graphs can be acyclic or contain loops, allowing for iterative refinement. |\\n| **State** | A shared, mutable context that nodes read from and write to. Think of it as a conversation memory that persists across steps. |\\n| **Executor** | The runtime that traverses the graph, resolving dependencies, handling async I/O, and managing retries. |\\n\\n---\\n\\n### How It Works\\n\\n1. **Define Nodes** ‚Äì Each node encapsulates a specific task. For example, a `Summarizer` node might take a chunk of text and return a concise summary; a `Retriever` node might query a vector store. Nodes are implemented as simple Python callables or subclasses of `BaseNode`, and can be decorated with `@langgraph.node`.\\n\\n2. **Connect Nodes** ‚Äì Edges are declared declaratively using the `@graph` decorator. You specify which node outputs feed into which inputs, and optionally provide conditions that determine the next node (e.g., if the summary is too short, loop back to the retriever).\\n\\n3. **Manage State** ‚Äì The graph‚Äôs state is a dictionary that persists across nodes. You can store intermediate results, flags, or even entire conversation histories. This shared state is crucial for multi‚Äëturn dialogue or chain‚Äëof‚Äëthought reasoning.\\n\\n4. **Execute** ‚Äì The `Executor` walks the graph from the designated start node, invoking each node in turn. It handles asynchronous execution, error handling, and can be paused or resumed. The final output is typically the value of a designated terminal node.\\n\\n---\\n\\n### Real‚ÄëWorld Use Cases\\n\\n- **Multi‚ÄëStep Question Answering** ‚Äì A graph can first retrieve relevant documents, then ask the LLM to extract key facts, and finally synthesize an answer. The state holds the retrieved snippets, making it easy to audit each step.\\n\\n- **Conversational Agents** ‚Äì By embedding a `Turn` node that captures user input and a `Response` node that generates replies, the graph can maintain context over long sessions. The state can store a conversation history that is fed back into the prompt.\\n\\n- **Data‚ÄëDriven Workflows** ‚Äì Combine LLM inference with external APIs. For instance, a node could call a weather API, another could ask the LLM to interpret the data, and a final node could generate a user‚Äëfriendly report.\\n\\n- **Iterative Refinement** ‚Äì Loops allow the system to refine outputs. A `Rewriter` node can improve the fluency of an LLM‚Äôs draft, and a `Validator` node can check for factual correctness, looping until a quality threshold is met.\\n\\n---\\n\\n### Advantages Over Traditional Prompt Pipelines\\n\\n1. **Modularity** ‚Äì Nodes can be reused across graphs. A `Summarizer` node built for one project can be dropped into another without modification.\\n\\n2. **Observability** ‚Äì Each node‚Äôs input and output can be logged, enabling fine‚Äëgrained debugging. GraphViz integration lets you visualize the entire workflow.\\n\\n3. **Scalability** ‚Äì Graphs can be distributed across multiple workers or even micro‚Äëservices. The executor can schedule nodes on GPU clusters or serverless functions.\\n\\n4. **Extensibility** ‚Äì Adding a new capability (e.g., a new retrieval backend) involves writing a single node and wiring it into the graph.\\n\\n5. **Declarative Flow Control** ‚Äì Conditional logic is expressed at the graph level, not inside prompts. This reduces prompt clutter and makes the control flow explicit.\\n\\n---\\n\\n### Getting Started\\n\\n```python\\nfrom langgraph import Graph, node, executor\\n\\n@node\\ndef retrieve(query: str) -> str:\\n    # Example: call a vector store\\n    return vector_store.search(query)\\n\\n@node\\ndef summarize(text: str) -> str:\\n    return llm.generate(f\"Summarize: {text}\")\\n\\n@node\\ndef answer(summary: str, question: str) -> str:\\n    return llm.generate(f\"Answer the question \\'{question}\\' based on: {summary}\")\\n\\ngraph = Graph()\\ngraph.add_node(retrieve, name=\"Retriever\")\\ngraph.add_node(summarize, name=\"Summarizer\")\\ngraph.add_node(answer, name=\"Answerer\")\\n\\ngraph.connect(\"Retriever\", \"Summarizer\", inputs={\"text\": \"output\"})\\ngraph.connect(\"Summarizer\", \"Answerer\", inputs={\"summary\": \"output\"})\\ngraph.set_start(\"Retriever\")\\n\\nexecutor = executor.Executor(graph)\\nresult = executor.run({\"query\": \"What caused the fall of the Roman Empire?\"})\\nprint(result[\"Answerer\"])\\n```\\n\\nRunning this snippet will trigger the retrieval, summarization, and answering steps automatically, with the state carrying data between them.\\n\\n---\\n\\n### Limitations & Future Directions\\n\\n- **Complex Graphs Can Be Hard to Visualize** ‚Äì While GraphViz helps, very large graphs may still be opaque. Tooling for hierarchical views is an active area of development.\\n\\n- **Learning Curve** ‚Äì New developers must grasp both LLM fundamentals and graph theory concepts. Comprehensive documentation and tutorials are expanding.\\n\\n- **State Management Overhead** ‚Äì Persisting large state objects can be memory‚Äëintensive. Future releases aim to integrate more efficient serialization and partial state updates.\\n\\n- **Integration with Other Frameworks** ‚Äì LangGraph is designed to interoperate with LangChain, but bridging to non‚ÄëPython ecosystems (e.g., JavaScript) will broaden its reach.\\n\\n---\\n\\n### Conclusion\\n\\nLangGraph represents a paradigm shift in how we build LLM‚Äëpowered applications. By treating every inference step as a node in a graph, it brings modularity, transparency, and scalability to the forefront. Whether you‚Äôre prototyping a chatbot, orchestrating a data‚Äëdriven decision engine, or building a multi‚Äëstep reasoning pipeline, LangGraph offers a flexible, declarative framework that makes complex LLM workflows manageable and maintainable. As the ecosystem matures, expect richer tooling, tighter integration with vector databases, and even more powerful abstractions that bring the full power of graph theory to the AI developer‚Äôs toolkit.' additional_kwargs={'reasoning_content': \"We need to produce a 500-word article on LangGraph. The user wants 500 words. We need to ensure we hit that length. Let's consider what LangGraph is: It's a framework for building language model applications with graph-based architectures, from LangChain. Provide details: architecture, components, use-cases, how it works, how to use, examples, advantages, limitations, future directions. Must be around 500 words. Let's target ~520 words to be safe. We'll write a coherent piece.\"} response_metadata={'token_usage': {'completion_tokens': 1549, 'prompt_tokens': 82, 'total_tokens': 1631, 'completion_time': 1.655516733, 'prompt_time': 0.032100409, 'queue_time': 0.16649755, 'total_time': 1.6876171420000001, 'completion_tokens_details': {'reasoning_tokens': 104}}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--fbfe8395-4cec-4231-b207-93ed2ad7a564-0' usage_metadata={'input_tokens': 82, 'output_tokens': 1549, 'total_tokens': 1631}\n"
     ]
    }
   ],
   "source": [
    "query=\"can you give me 500 words on langgraph?\"\n",
    "response =cache_model(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50bc19f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***CACHE MISS ‚Äì EXECUTING MODEL***\n",
      "EXECUTION TIME: 1.80 seconds\n",
      "content='**LangGraph: The Next‚ÄëGeneration Toolkit for Building LLM‚ÄëPowered Applications**\\n\\nLangGraph is a lightweight, open‚Äësource framework designed to make it easy to build, orchestrate, and debug complex workflows that combine large language models (LLMs) with external data sources, APIs, and custom logic. Think of it as the ‚ÄúApache Airflow‚Äù for language‚Äëmodel pipelines: it lets you describe a graph of nodes (functions, prompts, or calls to external services), specify the data that flows between them, and run the whole thing with a single command. Below is a deep dive into what LangGraph offers, why it matters, and how you can get started.\\n\\n---\\n\\n### 1. Why LangGraph Exists\\n\\nTraditional LLM applications are often built as monolithic scripts. While this works for quick experiments, it quickly becomes brittle as the number of prompts, retrieval steps, and decision points grows. LangGraph addresses three pain points:\\n\\n1. **Modularity** ‚Äì Each node is a self‚Äëcontained unit (e.g., ‚ÄúGenerate summary‚Äù, ‚ÄúCall weather API‚Äù, ‚ÄúValidate response‚Äù). You can swap, test, or update nodes without touching the rest of the pipeline.\\n2. **Explicit Data Flow** ‚Äì The framework exposes the input and output schema of each node, making it trivial to trace where a piece of data originates and where it will be used next.\\n3. **Observability & Debugging** ‚Äì Every step is logged, and the framework provides a visual graph that can be inspected in real time, helping developers identify bottlenecks or logic errors.\\n\\n---\\n\\n### 2. Core Concepts\\n\\n| Concept | What It Is | How It Helps |\\n|---------|------------|--------------|\\n| **Node** | A Python callable (function, class method, or external API wrapper). | Encapsulates a single operation; can be reused. |\\n| **Edge** | A directed connection that passes data from one node‚Äôs output to another‚Äôs input. | Explicitly defines the workflow. |\\n| **Graph** | A collection of nodes and edges forming a directed acyclic graph (DAG). | Represents the entire application logic. |\\n| **Context** | A shared dictionary that carries data across nodes. | Enables stateful operations without global variables. |\\n| **State Machine** | Optional: a finite‚Äëstate machine that drives the graph based on conditions. | Handles branching, loops, and error handling. |\\n\\n---\\n\\n### 3. Architecture Highlights\\n\\n- **Composable Nodes**: Nodes can be defined using decorators or simple function signatures. The framework automatically infers input and output types via type hints.\\n- **Plug‚Äëand‚ÄëPlay Prompts**: LangGraph supports prompt templates that can be parameterized and cached. You can switch between OpenAI, Anthropic, or any LLM that exposes a compatible API.\\n- **External Service Integration**: Built‚Äëin wrappers for common services (SQL databases, REST APIs, vector stores) reduce boilerplate.\\n- **Stateful Execution**: The graph runs in a single thread by default, but you can configure parallel execution for independent branches.\\n- **Observability**: A lightweight UI (built with FastAPI and React) visualizes the graph, shows real‚Äëtime logs, and lets you replay or modify nodes on the fly.\\n\\n---\\n\\n### 4. A Minimal Example\\n\\n```python\\nfrom langgraph import Graph, Node, Edge\\nfrom langchain.llms import OpenAI\\n\\n# 1. Define nodes\\n@Node\\ndef fetch_user_info(user_id: int) -> dict:\\n    # Imagine this hits a database\\n    return {\"name\": \"Alice\", \"city\": \"Paris\"}\\n\\n@Node\\ndef generate_summary(info: dict) -> str:\\n    prompt = f\"Summarize the following user info: {info}\"\\n    return OpenAI().complete(prompt)\\n\\n@Node\\ndef send_email(summary: str) -> None:\\n    print(f\"Sending email with body: {summary}\")\\n\\n# 2. Build graph\\ng = Graph(\\n    nodes=[fetch_user_info, generate_summary, send_email],\\n    edges=[\\n        Edge(fetch_user_info, generate_summary, input=\"info\"),\\n        Edge(generate_summary, send_email, input=\"summary\"),\\n    ]\\n)\\n\\n# 3. Execute\\ng.run({\"user_id\": 42})\\n```\\n\\nRunning the graph prints the email body, but the same graph can be expanded to add caching, error handling, or branching logic without touching the node definitions.\\n\\n---\\n\\n### 5. Advanced Features\\n\\n- **Conditional Branching**: Use `StateMachine` to decide between ‚Äúfriendly‚Äù and ‚Äúformal‚Äù email templates based on user preferences.\\n- **Retries & Back‚Äëoff**: Automatically retry failed external calls with exponential back‚Äëoff.\\n- **Dynamic Graphs**: Nodes can generate new edges on the fly, enabling adaptive workflows (e.g., if a user asks for a location-based recommendation, the graph can add a ‚ÄúCall Map API‚Äù node).\\n- **Unit Testing**: Because nodes are isolated, you can write unit tests that mock LLM outputs or external APIs.\\n\\n---\\n\\n### 6. Comparison with Alternatives\\n\\n| Feature | LangGraph | LangChain | LlamaIndex |\\n|---------|-----------|-----------|------------|\\n| Explicit graph visualization | ‚úî | ‚ùå | ‚ùå |\\n| Built‚Äëin state machine | ‚úî | ‚ùå | ‚ùå |\\n| Lightweight, no heavy runtime | ‚úî | ‚öñÔ∏è | ‚öñÔ∏è |\\n| Focus on LLM orchestration | ‚úî | ‚úî | ‚úî |\\n| Community & docs | Growing | Mature | Mature |\\n\\nLangGraph excels when you need a clean, observable workflow that can grow from a single function to a multi‚Äëstep, stateful application.\\n\\n---\\n\\n### 7. Getting Started\\n\\n1. **Install**  \\n   ```bash\\n   pip install langgraph\\n   ```\\n2. **Create a Graph** ‚Äì Define nodes, edges, and optional state machine.  \\n3. **Run** ‚Äì `graph.run(input_data)` or expose via FastAPI for real‚Äëtime interaction.  \\n4. **Visualize** ‚Äì Launch the built‚Äëin UI: `python -m langgraph.ui` and navigate to `http://localhost:8000`.\\n\\n---\\n\\n### 8. Future Roadmap\\n\\n- **Distributed Execution** ‚Äì Offload heavy nodes to a job queue (Celery, Prefect).  \\n- **Auto‚ÄëScaling** ‚Äì Dynamic scaling of LLM calls based on request load.  \\n- **Security Enhancements** ‚Äì Fine‚Äëgrained access control for nodes that access sensitive data.  \\n- **Marketplace** ‚Äì A repository of pre‚Äëbuilt nodes (e.g., ‚ÄúTranslate to Spanish‚Äù, ‚ÄúExtract entities from PDF‚Äù).\\n\\n---\\n\\n### 9. Conclusion\\n\\nLangGraph is more than a library; it‚Äôs a design pattern for building robust, maintainable LLM applications. By turning a workflow into a graph of explicit, testable nodes, developers can iterate faster, debug more efficiently, and scale their solutions with confidence. Whether you‚Äôre prototyping a chatbot, building a data‚Äëdriven recommendation engine, or orchestrating a multi‚Äëstep data‚Äëscience pipeline, LangGraph gives you the tools to keep your code clean, your logic transparent, and your applications resilient.' additional_kwargs={'reasoning_content': \"User wants 500 words on LangGraph. Likely a library for building LLM pipelines. Provide overview, architecture, usage, examples, pros/cons, future. Ensure 500 words. Let's craft ~500 words.\"} response_metadata={'token_usage': {'completion_tokens': 1528, 'prompt_tokens': 80, 'total_tokens': 1608, 'completion_time': 1.623438148, 'prompt_time': 0.005265957, 'queue_time': 0.046694063, 'total_time': 1.628704105, 'completion_tokens_details': {'reasoning_tokens': 46}}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--93f63ebe-61bb-4615-8bd0-b1e49b707e75-0' usage_metadata={'input_tokens': 80, 'output_tokens': 1528, 'total_tokens': 1608}\n"
     ]
    }
   ],
   "source": [
    "query=\"give me 500 words on langgraph?\"\n",
    "response =cache_model(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5083b6b7",
   "metadata": {},
   "source": [
    "### Advanced CAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf663496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import TypedDict, List, Optional\n",
    "import time\n",
    "\n",
    "# ---- LangGraph / LangChain ----\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# ---- FAISS vector stores ----\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bb0e1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= CONFIG =================\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384-dim\n",
    "VECTOR_DIM = 384\n",
    "\n",
    "LLM_MODEL = \"groq:openai/gpt-oss-20b\"\n",
    "\n",
    "LLM_TEMPERATURE = 0\n",
    "\n",
    "RETRIEVE_TOP_K = 4\n",
    "CACHE_TOP_K = 3\n",
    "\n",
    "CACHE_DISTANCE_THRESHOLD = 0.45\n",
    "\n",
    "# Optional TTL for cache entries (seconds). 0 = disabled.\n",
    "CACHE_TTL_SEC = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "298cde63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= STATE ==================\n",
    "class RAGState(TypedDict):\n",
    "    question: str\n",
    "    normalized_question: str\n",
    "    context_docs: List[Document]\n",
    "    answer: Optional[str]\n",
    "    citations: List[str]\n",
    "    cache_hit: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55558c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== GLOBALS ===================\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "EMBED = HuggingFaceEmbeddings(model_name=EMBED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9791f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- QA CACHE (EMPTY, SAFE INIT) -----\n",
    "qa_index = faiss.IndexFlatL2(VECTOR_DIM)  # distance; lower is better\n",
    "QA_CACHE = FAISS(\n",
    "    embedding_function=EMBED,\n",
    "    index=qa_index,\n",
    "    docstore=InMemoryDocstore({}),\n",
    "    index_to_docstore_id={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36b896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x20d53044d90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QA_CACHE\n",
    "\n",
    "# this is an empty faiss vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "080fa22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- RAG STORE (demo only) -----\n",
    "RAG_STORE = FAISS.from_texts(\n",
    "    texts=[\n",
    "        \"LangGraph lets you compose stateful LLM workflows as graphs.\",\n",
    "        \"In LangGraph, nodes can be cached; node caching memoizes outputs keyed by inputs for a TTL.\",\n",
    "        \"Retrieval-Augmented Generation (RAG) retrieves external context and injects it into prompts.\",\n",
    "        \"Semantic caching reuses prior answers when new questions are semantically similar.\"\n",
    "    ],\n",
    "    embedding=EMBED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0cc311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM = init_chat_model(model=LLM_MODEL, temperature=LLM_TEMPERATURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d43f16f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000020DCFA32ED0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000020DCFA32DD0>, model_name='openai/gpt-oss-20b', temperature=1e-08, model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "997e26d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ NODES ===================\n",
    "def normalize_query(state: RAGState) -> RAGState:\n",
    "    q = (state[\"question\"] or \"\").strip()\n",
    "    state[\"normalized_question\"] = q.lower()\n",
    "    return state\n",
    "\n",
    "def semantic_cache_lookup(state: RAGState) -> RAGState:\n",
    "    q = state[\"normalized_question\"]\n",
    "    state[\"cache_hit\"] = False  # default\n",
    "\n",
    "    if not q:\n",
    "        return state\n",
    "\n",
    "    # ‚úÖ Guard: FAISS crashes if ntotal == 0 and you ask for k>0\n",
    "    if getattr(QA_CACHE, \"index\", None) is None or QA_CACHE.index.ntotal == 0:\n",
    "        return state\n",
    "\n",
    "    # For FAISS L2 wrapper, this returns (Document, distance) with lower=better\n",
    "    hits = QA_CACHE.similarity_search_with_score(q, k=CACHE_TOP_K)\n",
    "    if not hits:\n",
    "        return state\n",
    "\n",
    "    best_doc, dist = hits[0]\n",
    "\n",
    "    # Optional TTL\n",
    "    if CACHE_TTL_SEC > 0:\n",
    "        ts = best_doc.metadata.get(\"ts\")\n",
    "        if ts is None or (time.time() - float(ts)) > CACHE_TTL_SEC:\n",
    "            return state\n",
    "\n",
    "    # L2 distance gate (lower = more similar)\n",
    "    if dist <= CACHE_DISTANCE_THRESHOLD:\n",
    "        cached_answer = best_doc.metadata.get(\"answer\")\n",
    "        if cached_answer:\n",
    "            state[\"answer\"] = cached_answer\n",
    "            state[\"citations\"] = [\"(cache)\"]\n",
    "            state[\"cache_hit\"] = True\n",
    "\n",
    "    return state\n",
    "\n",
    "def respond_from_cache(state: RAGState) -> RAGState:\n",
    "    return state\n",
    "\n",
    "def retrieve(state: RAGState) -> RAGState:\n",
    "    q = state[\"normalized_question\"]\n",
    "    docs = RAG_STORE.similarity_search(q, k=RETRIEVE_TOP_K)\n",
    "    state[\"context_docs\"] = docs\n",
    "    return state\n",
    "\n",
    "def generate(state: RAGState) -> RAGState:\n",
    "    q = state[\"question\"]\n",
    "    docs = state.get(\"context_docs\", [])\n",
    "    ctx = \"\\n\\n\".join([f\"[doc-{i}] {d.page_content}\" for i, d in enumerate(docs, start=1)])\n",
    "\n",
    "    system = (\n",
    "        \"You are a precise RAG assistant. Use the context when helpful. \"\n",
    "        # \"Cite with [doc-i] markers if you use a fact from the context.\"\n",
    "    )\n",
    "    user = f\"Question: {q}\\n\\nContext:\\n{ctx}\\n\\nWrite a concise answer with citations.\"\n",
    "\n",
    "    resp = LLM.invoke([{\"role\": \"system\", \"content\": system},\n",
    "                       {\"role\": \"user\", \"content\": user}])\n",
    "    state[\"answer\"] = resp.content\n",
    "    state[\"citations\"] = [f\"[doc-{i}]\" for i in range(1, len(docs) + 1)]\n",
    "    return state\n",
    "\n",
    "def cache_write(state: RAGState) -> RAGState:\n",
    "    q = state[\"normalized_question\"]\n",
    "    a = state.get(\"answer\")\n",
    "    if not q or not a:\n",
    "        return state\n",
    "\n",
    "    QA_CACHE.add_texts(\n",
    "        texts=[q],\n",
    "        metadatas=[{\n",
    "            \"answer\": a,\n",
    "            \"ts\": time.time(),\n",
    "        }]\n",
    "    )\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "462f44af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x0000020DCFA3BE50>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============== GRAPH WIRING ==============\n",
    "graph = StateGraph(RAGState)\n",
    "\n",
    "graph.add_node(\"normalize_query\", normalize_query)\n",
    "graph.add_node(\"semantic_cache_lookup\", semantic_cache_lookup)\n",
    "graph.add_node(\"respond_from_cache\", respond_from_cache)\n",
    "graph.add_node(\"retrieve\", retrieve)\n",
    "graph.add_node(\"generate\", generate)\n",
    "graph.add_node(\"cache_write\", cache_write)\n",
    "\n",
    "graph.set_entry_point(\"normalize_query\")\n",
    "graph.add_edge(\"normalize_query\", \"semantic_cache_lookup\")\n",
    "\n",
    "def _branch(state: RAGState) -> str:\n",
    "    return \"respond_from_cache\" if state.get(\"cache_hit\") else \"retrieve\"\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"semantic_cache_lookup\",\n",
    "    _branch,\n",
    "    {\n",
    "        \"respond_from_cache\": \"respond_from_cache\",\n",
    "        \"retrieve\": \"retrieve\"\n",
    "    }\n",
    ")\n",
    "\n",
    "graph.add_edge(\"respond_from_cache\", END)\n",
    "graph.add_edge(\"retrieve\", \"generate\")\n",
    "graph.add_edge(\"generate\", \"cache_write\")\n",
    "graph.add_edge(\"cache_write\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = graph.compile(checkpointer=memory)\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba9bde2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: LangGraph is a framework that lets you build stateful LLM workflows as directed graphs, where each node can be cached (memoizing outputs keyed by inputs for a TTL) to improve efficiency. It enables modular, reusable LLM pipelines that can incorporate retrieval‚Äëaugmented generation and semantic caching techniques.„Äêdoc‚Äë2„Äë„Äêdoc‚Äë1„Äë\n",
      "Citations: ['[doc-1]', '[doc-2]', '[doc-3]', '[doc-4]']\n",
      "Cache hit?: False\n"
     ]
    }
   ],
   "source": [
    "# ================= DEMO ===================\n",
    "if __name__ == \"__main__\":\n",
    "    thread_cfg = {\"configurable\": {\"thread_id\": \"demo-user-1\"}}\n",
    "\n",
    "    q1 = \"What is LangGraph ?\"\n",
    "    out1 = app.invoke({\"question\": q1, \"context_docs\": [], \"citations\": []}, thread_cfg)\n",
    "    print(\"Answer:\", out1[\"answer\"])\n",
    "    print(\"Citations:\", out1.get(\"citations\"))\n",
    "    print(\"Cache hit?:\", out1.get(\"cache_hit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2039bde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: LangGraph is a framework that lets you build stateful LLM workflows as directed graphs, where each node can be cached (memoizing outputs keyed by inputs for a TTL) to improve efficiency. It enables modular, reusable LLM pipelines that can incorporate retrieval‚Äëaugmented generation and semantic caching techniques.„Äêdoc‚Äë2„Äë„Äêdoc‚Äë1„Äë\n",
      "Citations: ['(cache)']\n",
      "Cache hit?: True\n"
     ]
    }
   ],
   "source": [
    "q1 = \"Explain about LangGraph ?\"\n",
    "out1 = app.invoke({\"question\": q1, \"context_docs\": [], \"citations\": []}, thread_cfg)\n",
    "print(\"Answer:\", out1[\"answer\"])\n",
    "print(\"Citations:\", out1.get(\"citations\"))\n",
    "print(\"Cache hit?:\", out1.get(\"cache_hit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c8d9021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: LangGraph agents are stateful LLM workflows built as directed graphs, where each node represents a step (e.g., prompt, retrieval, or transformation) that can be cached for efficiency„Äêdoc-2„Äë„Äêdoc-1„Äë. The framework supports Retrieval‚ÄëAugmented Generation (RAG), allowing agents to fetch external context and inject it into prompts„Äêdoc-3„Äë. Additionally, LangGraph can employ semantic caching to reuse prior answers when new queries are semantically similar, further speeding up responses„Äêdoc-4„Äë.\n",
      "Citations: ['[doc-1]', '[doc-2]', '[doc-3]', '[doc-4]']\n",
      "Cache hit?: False\n"
     ]
    }
   ],
   "source": [
    "q1 = \"Explain about LangGraph agents ?\"\n",
    "out1 = app.invoke({\"question\": q1, \"context_docs\": [], \"citations\": []}, thread_cfg)\n",
    "print(\"Answer:\", out1[\"answer\"])\n",
    "print(\"Citations:\", out1.get(\"citations\"))\n",
    "print(\"Cache hit?:\", out1.get(\"cache_hit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d890c065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: LangGraph agents are stateful LLM workflows built as directed graphs, where each node represents a step (e.g., prompt, retrieval, or transformation) that can be cached for efficiency„Äêdoc-2„Äë„Äêdoc-1„Äë. The framework supports Retrieval‚ÄëAugmented Generation (RAG), allowing agents to fetch external context and inject it into prompts„Äêdoc-3„Äë. Additionally, LangGraph can employ semantic caching to reuse prior answers when new queries are semantically similar, further speeding up responses„Äêdoc-4„Äë.\n",
      "Citations: ['(cache)']\n",
      "Cache hit?: True\n"
     ]
    }
   ],
   "source": [
    "q1 = \"Explain about agents in Langgraph ?\"\n",
    "out1 = app.invoke({\"question\": q1, \"context_docs\": [], \"citations\": []}, thread_cfg)\n",
    "print(\"Answer:\", out1[\"answer\"])\n",
    "print(\"Citations:\", out1.get(\"citations\"))\n",
    "print(\"Cache hit?:\", out1.get(\"cache_hit\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticai-langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
